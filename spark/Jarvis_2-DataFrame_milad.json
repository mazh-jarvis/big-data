{"paragraphs":[{"text":"%md\n## Learning Dataset/DataFrame\nReadings: \n\n- `Spark - The Definitive Guide` chapter 3 - 10\n- The official document https://spark.apache.org/docs/2.3.3/sql-programming-guide.html","user":"anonymous","dateUpdated":"2019-10-15T14:57:14+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Learning Dataset/DataFrame</h2>\n<p>Readings: </p>\n<ul>\n  <li><code>Spark - The Definitive Guide</code> chapter 3 - 10</li>\n  <li>The official document <a href=\"https://spark.apache.org/docs/2.3.3/sql-programming-guide.html\">https://spark.apache.org/docs/2.3.3/sql-programming-guide.html</a></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1570548466375_-1453420799","id":"20191007-143023_835288946","dateCreated":"2019-10-08T15:27:46+0000","dateStarted":"2019-10-15T14:57:14+0000","dateFinished":"2019-10-15T14:57:16+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:172"},{"text":"%md\n### Creating Dataframes\n#### Creating Dataframes from Scala `Seq`\n\nWe can convert a Sequence of Tuples to a Spark DF.\ne.g. Seq[(String, Double, String, String)] \n\nA tuple corresponds to a DF row.\nA element in a tuple corresponds to a column to a particular row.","user":"anonymous","dateUpdated":"2019-10-15T14:57:16+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Creating Dataframes</h3>\n<h4>Creating Dataframes from Scala <code>Seq</code></h4>\n<p>We can convert a Sequence of Tuples to a Spark DF.<br/>e.g. Seq[(String, Double, String, String)] </p>\n<p>A tuple corresponds to a DF row.<br/>A element in a tuple corresponds to a column to a particular row.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1570548466388_-953632780","id":"20190519-201210_1157722001","dateCreated":"2019-10-08T15:27:46+0000","dateStarted":"2019-10-15T14:57:20+0000","dateFinished":"2019-10-15T14:57:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:173"},{"text":"%md\n#### Scala Implicit Conversions\n\nIn short, you have to `import spark.implicits._` to convert/cast a `Seq[(String, Double, String, String)]` to a Spark `DataFrame`. (e.g. `lineTupleSeq.toDF`)\n\n#####  (Advanced)\nThis is called implicit conversions in Scala. In this case, `spark.implicits.localSeqToDatasetHolder` creates a Dataset from a local Seq.\n\nSpark Scala Docs:\n- <a href=\"https://spark.apache.org/docs/2.3.0/api/scala/index.html#org.apache.spark.sql.SparkSession$implicits$@localSeqToDatasetHolder[T](s:Seq[T])(implicitevidence$7:org.apache.spark.sql.Encoder[T]):org.apache.spark.sql.DatasetHolder[T]\" target=\"_blank\">implicits.localSeqToDatasetHolder</a>\n- <a href=\"http://spark.apache.org/docs/2.3.0/api/scala/index.html#org.apache.spark.sql.DatasetHolder@toDF(colNames:String*):org.apache.spark.sql.DataFrame\" target=\"_blank\">DatasetHolder</a>","user":"anonymous","dateUpdated":"2019-10-15T14:57:20+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Scala Implicit Conversions</h4>\n<p>In short, you have to <code>import spark.implicits._</code> to convert/cast a <code>Seq[(String, Double, String, String)]</code> to a Spark <code>DataFrame</code>. (e.g. <code>lineTupleSeq.toDF</code>)</p>\n<h5>(Advanced)</h5>\n<p>This is called implicit conversions in Scala. In this case, <code>spark.implicits.localSeqToDatasetHolder</code> creates a Dataset from a local Seq.</p>\n<p>Spark Scala Docs:<br/>- <a href=\"https://spark.apache.org/docs/2.3.0/api/scala/index.html#org.apache.spark.sql.SparkSession$implicits$@localSeqToDatasetHolder[T](s:Seq[T])(implicitevidence$7:org.apache.spark.sql.Encoder[T]):org.apache.spark.sql.DatasetHolder[T]\" target=\"_blank\">implicits.localSeqToDatasetHolder</a><br/>- <a href=\"http://spark.apache.org/docs/2.3.0/api/scala/index.html#org.apache.spark.sql.DatasetHolder@toDF(colNames:String*):org.apache.spark.sql.DataFrame\" target=\"_blank\">DatasetHolder</a></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1570548466389_-1263577688","id":"20190520-102917_1809142825","dateCreated":"2019-10-08T15:27:46+0000","dateStarted":"2019-10-15T14:57:20+0000","dateFinished":"2019-10-15T14:57:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:174"},{"text":"/**\n * Each line/record/row must be a Tuple\n * e.g.  Tuple(AAPL,110.5,2018-02-01,Apple)\n * \n * Lines are grouped into a Seq\n * List(\n *   (AAPL,110.5,2018-02-01,Apple),\n *   (AMZN,1500.52,2018-02-01,Ammazon.com),\n *   (FB,170.01,2018-02-01,Facebook)\n * )\n */\nval lineTuple1 = (\"AAPL\",110.5,\"2018-02-01\",\"Apple\")\nval lineTuple2 = (\"AMZN\",1500.52,\"2018-02-01\",\"Ammazon.com\")\nval lineTuple3 = (\"FB\",170.01,\"2018-02-01\",\"Facebook\")\nval lineTupleSeq = Seq(lineTuple1,lineTuple2,lineTuple3)\n\n//To use toDF, you must import this (see next section for details)\n//In fact Zeppellin interpreter already imported this for you\nimport spark.implicits._\nval stockDf = lineTupleSeq.toDF(\"ticker\",\"price\", \"date\", \"companyName\")\nstockDf.printSchema\n\n//SELECT * FROM stock LIMIT 3\nstockDf.show(3)\n\n//SELECT companyName AS company_name, price FROM stock\nstockDf.select(col(\"companyName\").as(\"company_name\"), col(\"price\")).show()\n\n","user":"anonymous","dateUpdated":"2019-10-15T14:57:20+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"lineTuple1: (String, Double, String, String) = (AAPL,110.5,2018-02-01,Apple)\nlineTuple2: (String, Double, String, String) = (AMZN,1500.52,2018-02-01,Ammazon.com)\nlineTuple3: (String, Double, String, String) = (FB,170.01,2018-02-01,Facebook)\nlineTupleSeq: Seq[(String, Double, String, String)] = List((AAPL,110.5,2018-02-01,Apple), (AMZN,1500.52,2018-02-01,Ammazon.com), (FB,170.01,2018-02-01,Facebook))\nimport spark.implicits._\nstockDf: org.apache.spark.sql.DataFrame = [ticker: string, price: double ... 2 more fields]\nroot\n |-- ticker: string (nullable = true)\n |-- price: double (nullable = false)\n |-- date: string (nullable = true)\n |-- companyName: string (nullable = true)\n\n+------+-------+----------+-----------+\n|ticker|  price|      date|companyName|\n+------+-------+----------+-----------+\n|  AAPL|  110.5|2018-02-01|      Apple|\n|  AMZN|1500.52|2018-02-01|Ammazon.com|\n|    FB| 170.01|2018-02-01|   Facebook|\n+------+-------+----------+-----------+\n\n+------------+-------+\n|company_name|  price|\n+------------+-------+\n|       Apple|  110.5|\n| Ammazon.com|1500.52|\n|    Facebook| 170.01|\n+------------+-------+\n\n"}]},"apps":[],"jobName":"paragraph_1570548466394_-1073185837","id":"20190519-201416_412351679","dateCreated":"2019-10-08T15:27:46+0000","dateStarted":"2019-10-15T14:57:21+0000","dateFinished":"2019-10-15T14:58:04+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:175"},{"text":"%md\n### Creating DF from CSV Files","user":"anonymous","dateUpdated":"2019-10-15T14:58:04+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Creating DF from CSV Files</h3>\n</div>"}]},"apps":[],"jobName":"paragraph_1570548466396_210342098","id":"20190520-104920_1833330750","dateCreated":"2019-10-08T15:27:46+0000","dateStarted":"2019-10-15T14:58:05+0000","dateFinished":"2019-10-15T14:58:05+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:176"},{"text":"//Read CSV file to df\n//local or hdfs path\n//val path = \"/user/milad/datasets/online_retail/online-retail-dataset_clean.txt\"\nval path = \"/user/milad/datasets/online_retail/online-retail-dataset.csv\"\n\n//spark.read is able to handle csv formats\nval retailDf = spark.read.format(\"csv\").\n    option(\"header\", \"true\").\n    option(\"inferSchema\", \"true\").\n    load(path)\n\n\nretailDf.printSchema\nretailDf.show(3)\nretailDf.show(3,false)","user":"anonymous","dateUpdated":"2019-10-15T15:15:49+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"path: String = /user/milad/datasets/online_retail/online-retail-dataset.csv\nretailDf: org.apache.spark.sql.DataFrame = [InvoiceNo: string, StockCode: string ... 6 more fields]\nroot\n |-- InvoiceNo: string (nullable = true)\n |-- StockCode: string (nullable = true)\n |-- Description: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- InvoiceDate: string (nullable = true)\n |-- UnitPrice: double (nullable = true)\n |-- CustomerID: integer (nullable = true)\n |-- Country: string (nullable = true)\n\n+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|\n+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\nonly showing top 3 rows\n\n+---------+---------+----------------------------------+--------+--------------+---------+----------+--------------+\n|InvoiceNo|StockCode|Description                       |Quantity|InvoiceDate   |UnitPrice|CustomerID|Country       |\n+---------+---------+----------------------------------+--------+--------------+---------+----------+--------------+\n|536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER|6       |12/1/2010 8:26|2.55     |17850     |United Kingdom|\n|536365   |71053    |WHITE METAL LANTERN               |6       |12/1/2010 8:26|3.39     |17850     |United Kingdom|\n|536365   |84406B   |CREAM CUPID HEARTS COAT HANGER    |8       |12/1/2010 8:26|2.75     |17850     |United Kingdom|\n+---------+---------+----------------------------------+--------+--------------+---------+----------+--------------+\nonly showing top 3 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://mazh-jrvs-bootcamp-m.us-east1-b.c.rich-suprstate-244018.internal:4040/jobs/job?id=24","http://mazh-jrvs-bootcamp-m.us-east1-b.c.rich-suprstate-244018.internal:4040/jobs/job?id=25","http://mazh-jrvs-bootcamp-m.us-east1-b.c.rich-suprstate-244018.internal:4040/jobs/job?id=26","http://mazh-jrvs-bootcamp-m.us-east1-b.c.rich-suprstate-244018.internal:4040/jobs/job?id=27"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1570548466396_397995737","id":"20190520-095229_630927102","dateCreated":"2019-10-08T15:27:46+0000","dateStarted":"2019-10-15T15:15:49+0000","dateFinished":"2019-10-15T15:15:53+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:177"},{"text":"%md\n#### Column Type Cast\nIn `retailDf` schema, `InvoiceDate` column data type is string. \n\nIn this practice, you need to cast `InvoiceDate` column to a Spark `timestamp` data type\n\n```bash\nresultDf.printSchema\nroot\n |-- InvoiceNo: string (nullable = true)\n |-- StockCode: string (nullable = true)\n |-- Description: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- InvoiceDate: timestamp (nullable = true) #cast string to timestamp\n |-- UnitPrice: double (nullable = true)\n |-- CustomerID: integer (nullable = true)\n |-- Country: string (nullable = true)\n```\n","user":"anonymous","dateUpdated":"2019-10-15T14:58:20+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Column Type Cast</h4>\n<p>In <code>retailDf</code> schema, <code>InvoiceDate</code> column data type is string. </p>\n<p>In this practice, you need to cast <code>InvoiceDate</code> column to a Spark <code>timestamp</code> data type</p>\n<pre><code class=\"bash\">resultDf.printSchema\nroot\n |-- InvoiceNo: string (nullable = true)\n |-- StockCode: string (nullable = true)\n |-- Description: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- InvoiceDate: timestamp (nullable = true) #cast string to timestamp\n |-- UnitPrice: double (nullable = true)\n |-- CustomerID: integer (nullable = true)\n |-- Country: string (nullable = true)\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1570548466397_-1540744380","id":"20190520-085947_2007764287","dateCreated":"2019-10-08T15:27:46+0000","dateStarted":"2019-10-15T14:58:21+0000","dateFinished":"2019-10-15T14:58:21+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:178"},{"text":"import org.apache.spark.sql.functions.to_timestamp\nval FMT = \"MM/dd/yy H:mm\"\nval retailCastDf = retailDf.withColumn(\"invoiceDate\", to_timestamp(retailDf(\"InvoiceDate\"), FMT))\nretailCastDf.printSchema\n\nretailCastDf.show(3)","user":"anonymous","dateUpdated":"2019-10-15T15:16:02+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.functions.to_timestamp\nFMT: String = MM/dd/yy H:mm\nretailCastDf: org.apache.spark.sql.DataFrame = [InvoiceNo: string, StockCode: string ... 6 more fields]\nroot\n |-- InvoiceNo: string (nullable = true)\n |-- StockCode: string (nullable = true)\n |-- Description: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- invoiceDate: timestamp (nullable = true)\n |-- UnitPrice: double (nullable = true)\n |-- CustomerID: integer (nullable = true)\n |-- Country: string (nullable = true)\n\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|        invoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|     17850|United Kingdom|\n|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|     17850|United Kingdom|\n|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|     17850|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 3 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://mazh-jrvs-bootcamp-m.us-east1-b.c.rich-suprstate-244018.internal:4040/jobs/job?id=28"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1570664123172_1474027640","id":"20191009-233523_1741676923","dateCreated":"2019-10-09T23:35:23+0000","dateStarted":"2019-10-15T15:16:02+0000","dateFinished":"2019-10-15T15:16:04+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:179"},{"text":"//Cache DF in memory since it will be accessed frequently\nretailCastDf.cache\n","user":"anonymous","dateUpdated":"2019-10-15T14:58:24+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res24: retailCastDf.type = [InvoiceNo: string, StockCode: string ... 6 more fields]\n"}]},"apps":[],"jobName":"paragraph_1570548466398_-624687758","id":"20190519-215300_721200493","dateCreated":"2019-10-08T15:27:46+0000","dateStarted":"2019-10-15T14:58:24+0000","dateFinished":"2019-10-15T14:58:25+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:180"},{"text":"%md\n### DataFrame SELECT\nImplement the following SQL queries using dataframe. Compare different select syntax.\n\n```sql\nSELECT *\nFROM retail\nLIMIT 3\n\nSELECT InvoiceNo\nFROM retail\n\nSELECT InvoiceNo as invoiceNo\nFROM retail\n\nSELECT max(UnitPrice) as max_unit_price\nFROM retail\n```","user":"anonymous","dateUpdated":"2019-10-15T14:58:25+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>DataFrame SELECT</h3>\n<p>Implement the following SQL queries using dataframe. Compare different select syntax.</p>\n<pre><code class=\"sql\">SELECT *\nFROM retail\nLIMIT 3\n\nSELECT InvoiceNo\nFROM retail\n\nSELECT InvoiceNo as invoiceNo\nFROM retail\n\nSELECT max(UnitPrice) as max_unit_price\nFROM retail\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1570548466399_-1735076365","id":"20190519-221054_1925024171","dateCreated":"2019-10-08T15:27:46+0000","dateStarted":"2019-10-15T14:58:25+0000","dateFinished":"2019-10-15T14:58:25+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:181"},{"text":"/*\n//SELECT * from retail limit 1;\nretailCastDf.show(1)\nimport org.apache.spark.sql.functions._\n//select InvoiceNo,CustomerID,Country from retail limit 1;\nretailCastDf.select(\"InvoiceNo\").show(1)\n\n//Different ways of select \nretailCastDf.select($\"InvoiceNo\").show(1)\nretailCastDf.select('InvoiceNo).show(1)\nretailCastDf.select(col(\"InvoiceNo\")).show(1)\nretailCastDf.select(retailCastDf.col(\"InvoiceNo\")).show(1)\nretailCastDf.select(expr(\"InvoiceNo\")).show(1)\n\n//ERROR: cannot mix \n//retailCastDf.select($\"InvoiceNo\", \"StockCode\").show(1)\n\n//expr or selectExpr is most powerful and close to SQL syntax\n//SELECT InvoiceNo as invoiceId from retail limit 1;\nretailCastDf.select(expr(\"InvoiceNo as invoiceId\")).show(1)\nretailCastDf.selectExpr(\"InvoiceNo as invoiceId\").show(1)\n\n//SELECT * from retail limit 1;\nretailCastDf.selectExpr(\"*\").show(1)\n\n//select max(UnitPrice) as maxUnitPrice from retail\nretailCastDf.selectExpr(\"max(UnitPrice) as maxUnitPrice\").show \n\n*/","user":"anonymous","dateUpdated":"2019-10-15T14:58:25+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":11,"editorMode":"ace/mode/scala","fontSize":9,"results":{"0":{"graph":{"mode":"table","height":98.011,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1570548466399_-1353497372","id":"20190519-211701_1956303781","dateCreated":"2019-10-08T15:27:46+0000","dateStarted":"2019-10-15T14:58:25+0000","dateFinished":"2019-10-15T14:58:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:182"},{"text":"%md\n### DataFrame filtering (WHERE)\n\nImplement the following SQL quries\n\n```sql\nSELECT *\nFROM retail\nWHERE InvoiceNo = 536365\nLIMIT 2\n```\n\nSample results\n```\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|     17850|United Kingdom|\n|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|     17850|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 2 rows\n```","user":"anonymous","dateUpdated":"2019-10-15T14:58:26+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>DataFrame filtering (WHERE)</h3>\n<p>Implement the following SQL quries</p>\n<pre><code class=\"sql\">SELECT *\nFROM retail\nWHERE InvoiceNo = 536365\nLIMIT 2\n</code></pre>\n<p>Sample results</p>\n<pre><code>+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|     17850|United Kingdom|\n|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|     17850|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 2 rows\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1570548466400_751928792","id":"20190519-221114_648626738","dateCreated":"2019-10-08T15:27:46+0000","dateStarted":"2019-10-15T14:58:26+0000","dateFinished":"2019-10-15T14:58:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:183"},{"text":"retailCastDf.where($\"InvoiceNo\" === 536365).show(2)","user":"anonymous","dateUpdated":"2019-10-15T14:58:26+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|        invoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|     17850|United Kingdom|\n|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|     17850|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 2 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://mazh-jrvs-bootcamp-m.us-east1-b.c.rich-suprstate-244018.internal:4040/jobs/job?id=5"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1570548466401_-643543776","id":"20190519-201625_2028882244","dateCreated":"2019-10-08T15:27:46+0000","dateStarted":"2019-10-15T14:58:26+0000","dateFinished":"2019-10-15T14:58:32+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:184"},{"user":"anonymous","dateUpdated":"2019-10-15T14:58:32+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1570548466402_-1292323183","id":"20191007-145852_244125478","dateCreated":"2019-10-08T15:27:46+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:185"},{"text":"%md\n\n### DF Exercises\n#### Spark SQL temp view\nFor the following DF exercises, instead of jumping right into DF solutions, you can write `sql` solutions and verify with Spark SQL Temp Views.","user":"anonymous","dateUpdated":"2019-10-15T14:58:33+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>DF Exercises</h3>\n<h4>Spark SQL temp view</h4>\n<p>For the following DF exercises, instead of jumping right into DF solutions, you can write <code>sql</code> solutions and verify with Spark SQL Temp Views.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1570548466402_825430912","id":"20190520-123428_698724288","dateCreated":"2019-10-08T15:27:46+0000","dateStarted":"2019-10-15T14:58:33+0000","dateFinished":"2019-10-15T14:58:33+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:186"},{"text":"","user":"anonymous","dateUpdated":"2019-10-15T14:58:33+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|\n|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n|   536365|    22752|SET 7 BABUSHKA NE...|       2|12/1/2010 8:26|     7.65|     17850|United Kingdom|\n|   536365|    21730|GLASS STAR FROSTE...|       6|12/1/2010 8:26|     4.25|     17850|United Kingdom|\n+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n\n"}]},"apps":[],"jobName":"paragraph_1570548466403_1654884309","id":"20190520-142038_1683726413","dateCreated":"2019-10-08T15:27:46+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:187"},{"text":"%md\n#### Q1: Find the top N largest invoices by the amount (`Quantity * UnitPrice`)\n\nNote: `InvoiceNo` will appear in multiple rows. <br>(e.g. a receipt can have multiple items on it.)\n\n**Sample output**\n```bash\n+---------+------------------+\n|InvoiceNo|            Amount|\n+---------+------------------+\n|   581483|          168469.6|\n|   541431|           77183.6|\n|   574941| 52940.93999999999|\n|   576365|50653.909999999996|\n|   556444|           38970.0|\n+---------+------------------+\n```","user":"anonymous","dateUpdated":"2019-10-15T14:58:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Q1: Find the top N largest invoices by the amount (<code>Quantity * UnitPrice</code>)</h4>\n<p>Note: <code>InvoiceNo</code> will appear in multiple rows. <br>(e.g. a receipt can have multiple items on it.)</p>\n<p><strong>Sample output</strong></p>\n<pre><code class=\"bash\">+---------+------------------+\n|InvoiceNo|            Amount|\n+---------+------------------+\n|   581483|          168469.6|\n|   541431|           77183.6|\n|   574941| 52940.93999999999|\n|   576365|50653.909999999996|\n|   556444|           38970.0|\n+---------+------------------+\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1570548466403_284923334","id":"20190520-133812_405266917","dateCreated":"2019-10-08T15:27:46+0000","dateStarted":"2019-10-15T14:58:34+0000","dateFinished":"2019-10-15T14:58:34+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:188"},{"text":"","user":"anonymous","dateUpdated":"2019-10-15T14:58:34+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---------+------------------+\n|InvoiceNo|            Amount|\n+---------+------------------+\n|   581483|          168469.6|\n|   541431|           77183.6|\n|   574941| 52940.93999999999|\n|   576365|50653.909999999996|\n|   556444|           38970.0|\n+---------+------------------+\nonly showing top 5 rows\n\nlargestInvoicesDf: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [InvoiceNo: string, Amount: double]\n"}]},"apps":[],"jobName":"paragraph_1570548466404_1505010684","id":"20190519-215312_1016690251","dateCreated":"2019-10-08T15:27:46+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:189"},{"text":"// Create Retail View from refined dataframe\nval retailDf_R = retailCastDf.where($\"Quantity\" > 0).withColumn(\"Amount\", col(\"Quantity\")*col(\"UnitPrice\"))\n\nretailDf_R.createOrReplaceTempView(\"retail_view\")","user":"anonymous","dateUpdated":"2019-10-15T18:46:40+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"retailDf_R: org.apache.spark.sql.DataFrame = [InvoiceNo: string, StockCode: string ... 7 more fields]\n"}]},"apps":[],"jobName":"paragraph_1570720266708_357910247","id":"20191010-151106_268118940","dateCreated":"2019-10-10T15:11:06+0000","dateStarted":"2019-10-15T18:46:40+0000","dateFinished":"2019-10-15T18:46:42+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:190"},{"text":"// Investigate why my results defer from edwards\nspark.sql(\"SELECT * FROM retail_view WHERE InvoiceNo = 574941 ORDER BY Amount DESC\").show(5)\n\n// concl: Edward is using a different csv file","user":"anonymous","dateUpdated":"2019-10-15T15:27:37+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-------+\n|InvoiceNo|StockCode|         Description|Quantity|        invoiceDate|UnitPrice|CustomerID|       Country| Amount|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-------+\n|   574941|    22197|      POPCORN HOLDER|    1820|2011-11-07 17:42:00|     1.95|      null|United Kingdom| 3549.0|\n|   574941|    22086|PAPER CHAIN KIT 5...|     478|2011-11-07 17:42:00|     6.95|      null|United Kingdom| 3322.1|\n|   574941|    23084|  RABBIT NIGHT LIGHT|     628|2011-11-07 17:42:00|     4.95|      null|United Kingdom| 3108.6|\n|   574941|    23344|JUMBO BAG 50'S CH...|     484|2011-11-07 17:42:00|     4.95|      null|United Kingdom| 2395.8|\n|   574941|    23203|JUMBO BAG VINTAGE...|     375|2011-11-07 17:42:00|     4.95|      null|United Kingdom|1856.25|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-------+\nonly showing top 5 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://mazh-jrvs-bootcamp-m.us-east1-b.c.rich-suprstate-244018.internal:4040/jobs/job?id=35"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1571152792250_1106025892","id":"20191015-151952_851565882","dateCreated":"2019-10-15T15:19:52+0000","dateStarted":"2019-10-15T15:25:28+0000","dateFinished":"2019-10-15T15:25:30+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:191"},{"text":"\nspark.sql(\"SELECT InvoiceNo, Amount \" +\n    \"FROM retail_view \" +\n    \"ORDER BY Amount DESC \" +\n    \"LIMIT 5\").\n    show()","user":"anonymous","dateUpdated":"2019-10-15T15:17:54+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---------+--------+\n|InvoiceNo|  Amount|\n+---------+--------+\n|   581483|168469.6|\n|   541431| 77183.6|\n|   556444| 38970.0|\n|   537632|13541.33|\n|  A563185|11062.06|\n+---------+--------+\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://mazh-jrvs-bootcamp-m.us-east1-b.c.rich-suprstate-244018.internal:4040/jobs/job?id=30"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1570548466404_-446177243","id":"20191007-145909_914572499","dateCreated":"2019-10-08T15:27:46+0000","dateStarted":"2019-10-15T15:17:55+0000","dateFinished":"2019-10-15T15:17:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:192"},{"text":"%md\n#### Q2: Find the top N largest invoices by the amount and show receipt details\n\n```\n+---------+------------------+-------------------+----------+--------------+\n|InvoiceNo|            Amount|        InvoiceDate|CustomerID|       Country|\n+---------+------------------+-------------------+----------+--------------+\n|   581483|          168469.6|2011-12-09 09:15:00|     16446|United Kingdom|\n|   541431|           77183.6|2011-01-18 10:01:00|     12346|United Kingdom|\n|   574941| 52940.93999999999|2011-11-07 17:42:00|      null|United Kingdom|\n|   576365|50653.909999999996|2011-11-14 17:55:00|      null|United Kingdom|\n|   556444|           38970.0|2011-06-10 15:28:00|     15098|United Kingdom|\n+---------+------------------+-------------------+----------+--------------+\n```","user":"anonymous","dateUpdated":"2019-10-15T14:58:42+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Q2: Find the top N largest invoices by the amount and show receipt details</h4>\n<pre><code>+---------+------------------+-------------------+----------+--------------+\n|InvoiceNo|            Amount|        InvoiceDate|CustomerID|       Country|\n+---------+------------------+-------------------+----------+--------------+\n|   581483|          168469.6|2011-12-09 09:15:00|     16446|United Kingdom|\n|   541431|           77183.6|2011-01-18 10:01:00|     12346|United Kingdom|\n|   574941| 52940.93999999999|2011-11-07 17:42:00|      null|United Kingdom|\n|   576365|50653.909999999996|2011-11-14 17:55:00|      null|United Kingdom|\n|   556444|           38970.0|2011-06-10 15:28:00|     15098|United Kingdom|\n+---------+------------------+-------------------+----------+--------------+\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1570548466404_2056762298","id":"20190520-124355_215736883","dateCreated":"2019-10-08T15:27:46+0000","dateStarted":"2019-10-15T14:58:42+0000","dateFinished":"2019-10-15T14:58:42+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:193"},{"text":"\nspark.sql(\"SELECT InvoiceNo, Amount, InvoiceDate, CustomerID, Country \" +\n    \"FROM retail_view \" +\n    \"ORDER BY Amount DESC \" +\n    \"LIMIT 5\").\n    show()","user":"anonymous","dateUpdated":"2019-10-15T15:29:19+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---------+--------+-------------------+----------+--------------+\n|InvoiceNo|  Amount|        InvoiceDate|CustomerID|       Country|\n+---------+--------+-------------------+----------+--------------+\n|   581483|168469.6|2011-12-09 09:15:00|     16446|United Kingdom|\n|   541431| 77183.6|2011-01-18 10:01:00|     12346|United Kingdom|\n|   556444| 38970.0|2011-06-10 15:28:00|     15098|United Kingdom|\n|   537632|13541.33|2010-12-07 15:08:00|      null|United Kingdom|\n|  A563185|11062.06|2011-08-12 14:50:00|      null|United Kingdom|\n+---------+--------+-------------------+----------+--------------+\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://mazh-jrvs-bootcamp-m.us-east1-b.c.rich-suprstate-244018.internal:4040/jobs/job?id=36"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1570548466405_-1986934346","id":"20190520-122626_1736024345","dateCreated":"2019-10-08T15:27:46+0000","dateStarted":"2019-10-15T15:29:20+0000","dateFinished":"2019-10-15T15:29:22+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:194"},{"text":"%md\n#### Q3: For each country, find the top N largest invoices by the amount and show receipt details\n\nUse `Window functions` and `rank()` function\n\nReadings:\n- https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html\n- https://stackoverflow.com/questions/42966590/how-do-we-rank-dataframe\n- http://blog.madhukaraphatak.com/introduction-to-spark-two-part-5/\n- `Spark The Definitive Guide - page 134 - Windows Function`\n\n```\n+---------+------------------+-------------------+----------+---------+\n|InvoiceNo|            amount|        InvoiceDate|CustomerID|  Country|\n+---------+------------------+-------------------+----------+---------+\n|   571318| 5296.960000000001|2011-10-17 10:50:00|     17404|   Sweden|\n|   546530| 4400.280000000001|2011-03-14 13:25:00|     17404|   Sweden|\n|   571751|6068.0599999999995|2011-10-19 11:18:00|     12744|Singapore|\n|   548813|4037.7700000000004|2011-04-04 13:03:00|     12744|Singapore|\n|   552978| 9341.260000000004|2011-05-12 14:46:00|     12590|  Germany|\n|   564856|4257.0599999999995|2011-08-31 09:11:00|     12477|  Germany|\n|   571035|1002.3099999999998|2011-10-13 12:50:00|     12446|      RSA|\n|   573153| 8895.760000000004|2011-10-28 07:39:00|     12678|   France|\n|   570672| 4279.710000000004|2011-10-11 14:52:00|     12536|   France|\n|   541932|           2661.24|2011-01-24 11:39:00|     14439|   Greece|\n+---------+------------------+-------------------+----------+---------+\n```\n\n<br>\n<br>\n################ spoiler alert ################\n**Hints**:\n- At high level, you need to create a new column which indicates amount rank by country\n  - Use `Windows` function which partition by (\"Country\") and order by amount\n  - User `Rank()` function create a new `rank` column for each row\n  - filter out rows where `rank > 2`","user":"anonymous","dateUpdated":"2019-10-15T14:58:43+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Q3: For each country, find the top N largest invoices by the amount and show receipt details</h4>\n<p>Use <code>Window functions</code> and <code>rank()</code> function</p>\n<p>Readings:<br/>- <a href=\"https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html\">https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html</a><br/>- <a href=\"https://stackoverflow.com/questions/42966590/how-do-we-rank-dataframe\">https://stackoverflow.com/questions/42966590/how-do-we-rank-dataframe</a><br/>- <a href=\"http://blog.madhukaraphatak.com/introduction-to-spark-two-part-5/\">http://blog.madhukaraphatak.com/introduction-to-spark-two-part-5/</a><br/>- <code>Spark The Definitive Guide - page 134 - Windows Function</code></p>\n<pre><code>+---------+------------------+-------------------+----------+---------+\n|InvoiceNo|            amount|        InvoiceDate|CustomerID|  Country|\n+---------+------------------+-------------------+----------+---------+\n|   571318| 5296.960000000001|2011-10-17 10:50:00|     17404|   Sweden|\n|   546530| 4400.280000000001|2011-03-14 13:25:00|     17404|   Sweden|\n|   571751|6068.0599999999995|2011-10-19 11:18:00|     12744|Singapore|\n|   548813|4037.7700000000004|2011-04-04 13:03:00|     12744|Singapore|\n|   552978| 9341.260000000004|2011-05-12 14:46:00|     12590|  Germany|\n|   564856|4257.0599999999995|2011-08-31 09:11:00|     12477|  Germany|\n|   571035|1002.3099999999998|2011-10-13 12:50:00|     12446|      RSA|\n|   573153| 8895.760000000004|2011-10-28 07:39:00|     12678|   France|\n|   570672| 4279.710000000004|2011-10-11 14:52:00|     12536|   France|\n|   541932|           2661.24|2011-01-24 11:39:00|     14439|   Greece|\n+---------+------------------+-------------------+----------+---------+\n</code></pre>\n<br>\n<br>\n<p>################ spoiler alert ################<br/><strong>Hints</strong>:<br/>- At high level, you need to create a new column which indicates amount rank by country<br/> - Use <code>Windows</code> function which partition by (&ldquo;Country&rdquo;) and order by amount<br/> - User <code>Rank()</code> function create a new <code>rank</code> column for each row<br/> - filter out rows where <code>rank &gt; 2</code></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1570548466406_-1705619841","id":"20190520-150543_915955507","dateCreated":"2019-10-08T15:27:46+0000","dateStarted":"2019-10-15T14:58:43+0000","dateFinished":"2019-10-15T14:58:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:195"},{"text":"%spark\n\n/*spark.sql(\"desc table retail_view\").show\nspark.sql(\"select * from retail_view order by unitprice desc\").show(10)*/","user":"anonymous","dateUpdated":"2019-10-15T15:32:37+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-----------+---------+-------+\n|   col_name|data_type|comment|\n+-----------+---------+-------+\n|  InvoiceNo|   string|   null|\n|  StockCode|   string|   null|\n|Description|   string|   null|\n|   Quantity|      int|   null|\n|invoiceDate|timestamp|   null|\n|  UnitPrice|   double|   null|\n| CustomerID|      int|   null|\n|    Country|   string|   null|\n|     Amount|   double|   null|\n+-----------+---------+-------+\n\n+---------+---------+---------------+--------+-------------------+---------+----------+--------------+---------+\n|InvoiceNo|StockCode|    Description|Quantity|        invoiceDate|UnitPrice|CustomerID|       Country|   Amount|\n+---------+---------+---------------+--------+-------------------+---------+----------+--------------+---------+\n|  C556445|        M|         Manual|      -1|2011-06-10 15:31:00|  38970.0|     15098|United Kingdom| -38970.0|\n|  C580605|AMAZONFEE|     AMAZON FEE|      -1|2011-12-05 11:36:00| 17836.46|      null|United Kingdom|-17836.46|\n|  C540117|AMAZONFEE|     AMAZON FEE|      -1|2011-01-05 09:55:00| 16888.02|      null|United Kingdom|-16888.02|\n|  C540118|AMAZONFEE|     AMAZON FEE|      -1|2011-01-05 09:57:00| 16453.71|      null|United Kingdom|-16453.71|\n|  C537630|AMAZONFEE|     AMAZON FEE|      -1|2010-12-07 15:04:00| 13541.33|      null|United Kingdom|-13541.33|\n|   537632|AMAZONFEE|     AMAZON FEE|       1|2010-12-07 15:08:00| 13541.33|      null|United Kingdom| 13541.33|\n|  C537651|AMAZONFEE|     AMAZON FEE|      -1|2010-12-07 15:49:00| 13541.33|      null|United Kingdom|-13541.33|\n|  C537644|AMAZONFEE|     AMAZON FEE|      -1|2010-12-07 15:34:00| 13474.79|      null|United Kingdom|-13474.79|\n|  C580604|AMAZONFEE|     AMAZON FEE|      -1|2011-12-05 11:35:00|  11586.5|      null|United Kingdom| -11586.5|\n|  A563185|        B|Adjust bad debt|       1|2011-08-12 14:50:00| 11062.06|      null|United Kingdom| 11062.06|\n+---------+---------+---------------+--------+-------------------+---------+----------+--------------+---------+\nonly showing top 10 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://mazh-jrvs-bootcamp-m.us-east1-b.c.rich-suprstate-244018.internal:4040/jobs/job?id=11"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1571151031062_2034913363","id":"20191015-145031_289172180","dateCreated":"2019-10-15T14:50:31+0000","dateStarted":"2019-10-15T14:58:57+0000","dateFinished":"2019-10-15T14:58:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:196"},{"text":"import org.apache.spark.sql.expressions.Window\nimport org.apache.spark.sql.functions._\n/*\nspark.sql(\"SELECT InvoiceNo, Amount, InvoiceDate, CustomerID, Country, \" +\n    \"RANK() OVER (PARTITION BY Country ORDER BY Amount DESC) rank \" +\n    \"FROM retail_view \" + \n    \"HAVING rank < 3\" ).\n    show(10)\n*/\n\n// DF solution*\nval wSpec = Window.partitionBy(\"Country\").orderBy(desc(\"Amount\"))\n\nval q3df = retailDf_R.select($\"InvoiceNo\",\n    ($\"Amount\").alias(\"Amount\"),\n    $\"InvoiceDate\",\n    $\"CustomerID\",\n    $\"Country\")\n    \nq3df.withColumn(\"rank\", rank().over(wSpec)).\n    where($\"rank\" < 3).\n    show(10)\n","user":"anonymous","dateUpdated":"2019-10-15T18:47:32+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.expressions.Window\nimport org.apache.spark.sql.functions._\nwSpec: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@4f4b3df7\nq3df: org.apache.spark.sql.DataFrame = [InvoiceNo: string, Amount: double ... 3 more fields]\n+---------+-------+-------------------+----------+---------+----+\n|InvoiceNo| Amount|        InvoiceDate|CustomerID|  Country|rank|\n+---------+-------+-------------------+----------+---------+----+\n|   538848| 1188.0|2010-12-14 13:28:00|     17404|   Sweden|   1|\n|   566494|  792.0|2011-09-13 11:12:00|     17404|   Sweden|   2|\n|   571751|3949.32|2011-10-19 11:18:00|     12744|Singapore|   1|\n|   548813|2382.92|2011-04-04 13:03:00|     12744|Singapore|   2|\n|   569640|  876.0|2011-10-05 12:25:00|     12471|  Germany|   1|\n|   581179|  700.8|2011-12-07 15:43:00|     12471|  Germany|   2|\n|   571035|  38.25|2011-10-13 12:50:00|     12446|      RSA|   1|\n|   571035|   29.9|2011-10-13 12:50:00|     12446|      RSA|   2|\n|   573077|4161.06|2011-10-27 14:13:00|     12536|   France|   1|\n|   573080|4161.06|2011-10-27 14:20:00|     12536|   France|   1|\n+---------+-------+-------------------+----------+---------+----+\nonly showing top 10 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://mazh-jrvs-bootcamp-m.us-east1-b.c.rich-suprstate-244018.internal:4040/jobs/job?id=86","http://mazh-jrvs-bootcamp-m.us-east1-b.c.rich-suprstate-244018.internal:4040/jobs/job?id=87","http://mazh-jrvs-bootcamp-m.us-east1-b.c.rich-suprstate-244018.internal:4040/jobs/job?id=88","http://mazh-jrvs-bootcamp-m.us-east1-b.c.rich-suprstate-244018.internal:4040/jobs/job?id=89"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1570721201062_2058384578","id":"20191010-152641_364243002","dateCreated":"2019-10-10T15:26:41+0000","dateStarted":"2019-10-15T18:47:32+0000","dateFinished":"2019-10-15T18:47:38+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:197"},{"text":"%md\n\n#### Q4: Generate a daily and a weekly sales table and plot diagrams using Zeppelin built-in plot.\n\n\n```bash\ndailyDf.show(5)\n+-------------------+------------------+\n|              start|       sum(amount)|\n+-------------------+------------------+\n|2010-11-30 19:00:00|          58833.88|\n|2010-12-01 19:00:00| 45666.62999999999|\n|2010-12-02 19:00:00| 46161.11000000004|\n|2010-12-04 19:00:00|31383.949999999997|\n|2010-12-05 19:00:00| 53860.18000000004|\n+-------------------+------------------+\n```\n\n```bash\nweeklyDf.show(5)\n+-------------------+------------------+\n|              start|       sum(amount)|\n+-------------------+------------------+\n|2010-11-24 19:00:00| 58833.88000000002|\n|2010-12-01 19:00:00|         266320.76|\n|2010-12-08 19:00:00|234844.27999999997|\n|2010-12-15 19:00:00|177360.10999999993|\n|2010-12-22 19:00:00|11796.309999999992|\n+-------------------+------------------+\n```\n\nReadings\n- https://databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html\n- http://blog.madhukaraphatak.com/introduction-to-spark-two-part-5/","user":"anonymous","dateUpdated":"2019-10-08T15:27:46+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Q4: Generate a daily and a weekly sales table and plot diagrams using Zeppelin built-in plot.</h4>\n<pre><code class=\"bash\">dailyDf.show(5)\n+-------------------+------------------+\n|              start|       sum(amount)|\n+-------------------+------------------+\n|2010-11-30 19:00:00|          58833.88|\n|2010-12-01 19:00:00| 45666.62999999999|\n|2010-12-02 19:00:00| 46161.11000000004|\n|2010-12-04 19:00:00|31383.949999999997|\n|2010-12-05 19:00:00| 53860.18000000004|\n+-------------------+------------------+\n</code></pre>\n<pre><code class=\"bash\">weeklyDf.show(5)\n+-------------------+------------------+\n|              start|       sum(amount)|\n+-------------------+------------------+\n|2010-11-24 19:00:00| 58833.88000000002|\n|2010-12-01 19:00:00|         266320.76|\n|2010-12-08 19:00:00|234844.27999999997|\n|2010-12-15 19:00:00|177360.10999999993|\n|2010-12-22 19:00:00|11796.309999999992|\n+-------------------+------------------+\n</code></pre>\n<p>Readings<br/>- <a href=\"https://databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html\">https://databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html</a><br/>- <a href=\"http://blog.madhukaraphatak.com/introduction-to-spark-two-part-5/\">http://blog.madhukaraphatak.com/introduction-to-spark-two-part-5/</a></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1570548466407_-732817004","id":"20190520-140931_1510736707","dateCreated":"2019-10-08T15:27:46+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:199"},{"text":"// milad - df solution\nval dailyDf = retailDf_R.\n    groupBy(window($\"InvoiceDate\", \"1 day\") as \"start\").\n    agg(sum($\"Amount\") as \"sum(amount)\").\n    orderBy($\"start\")\n\nval weeklyDf = retailDf_R.\n    groupBy(window($\"InvoiceDate\", \"1 week\") as \"start\").\n    agg(sum($\"Amount\") as \"sum(amount)\").\n    orderBy($\"start\")\n\ndailyDf.show(5)\nweeklyDf.show(5)\n\ndailyDf.createOrReplaceTempView(\"dailySales\")\ndailyDf.createOrReplaceTempView(\"weeklySales\")","user":"anonymous","dateUpdated":"2019-10-15T18:54:47+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"dailyDf: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [start: struct<start: timestamp, end: timestamp>, sum(amount): double]\nweeklyDf: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [start: struct<start: timestamp, end: timestamp>, sum(amount): double]\n+--------------------+------------------+\n|               start|       sum(amount)|\n+--------------------+------------------+\n|[2010-12-01 00:00...| 58960.79000000025|\n|[2010-12-02 00:00...| 47748.37999999988|\n|[2010-12-03 00:00...| 46943.70999999988|\n|[2010-12-05 00:00...|31774.950000000164|\n|[2010-12-06 00:00...|54830.460000000014|\n+--------------------+------------------+\nonly showing top 5 rows\n\n+--------------------+------------------+\n|               start|       sum(amount)|\n+--------------------+------------------+\n|[2010-11-25 00:00...| 58960.79000000025|\n|[2010-12-02 00:00...|326305.68000000296|\n|[2010-12-09 00:00...|243804.23000000237|\n|[2010-12-16 00:00...| 182599.3300000008|\n|[2010-12-23 00:00...|12076.110000000024|\n+--------------------+------------------+\nonly showing top 5 rows\n\n"}]},"apps":[],"jobName":"paragraph_1570548466408_1358866718","id":"20190520-181045_1661878813","dateCreated":"2019-10-08T15:27:46+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:200","dateFinished":"2019-10-15T18:55:00+0000","dateStarted":"2019-10-15T18:54:47+0000","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://mazh-jrvs-bootcamp-m.us-east1-b.c.rich-suprstate-244018.internal:4040/jobs/job?id=96","http://mazh-jrvs-bootcamp-m.us-east1-b.c.rich-suprstate-244018.internal:4040/jobs/job?id=97"],"interpreterSettingId":"spark"}}},{"text":"// TODO: left off\n%sql\nselect to_date(start), `sum(amount)` from dailySales","user":"anonymous","dateUpdated":"2019-10-15T18:54:46+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{"0":{"graph":{"mode":"lineChart","height":326,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"start":"string","sum(amount)":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false},"multiBarChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"},"lineChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"},"stackedAreaChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"},"pieChart":{}},"commonSetting":{},"keys":[{"name":"to_date(dailysales.`start`)","index":0,"aggr":"sum"}],"groups":[],"values":[{"name":"sum(amount)","index":1,"aggr":"sum"}]},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"cannot resolve 'CAST(dailysales.`start` AS DATE)' due to data type mismatch: cannot cast struct<start:timestamp,end:timestamp> to date; line 1 pos 7;\n'Project [unresolvedalias(to_date(start#2710, None), None), sum(amount)#2721]\n+- SubqueryAlias dailysales\n   +- Sort [start#2710 ASC NULLS FIRST], true\n      +- Aggregate [window#2722], [window#2722 AS start#2710, sum(Amount#2515) AS sum(amount)#2721]\n         +- Filter isnotnull(InvoiceDate#1278)\n            +- Project [named_struct(start, precisetimestampconversion(((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(InvoiceDate#1278, TimestampType, LongType) - 0) as double) / cast(86400000000 as double))) as double) = (cast((precisetimestampconversion(InvoiceDate#1278, TimestampType, LongType) - 0) as double) / cast(86400000000 as double))) THEN (CEIL((cast((precisetimestampconversion(InvoiceDate#1278, TimestampType, LongType) - 0) as double) / cast(86400000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(InvoiceDate#1278, TimestampType, LongType) - 0) as double) / cast(86400000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 86400000000) + 0), LongType, TimestampType), end, precisetimestampconversion((((((CASE WHEN (cast(CEIL((cast((precisetimestampconversion(InvoiceDate#1278, TimestampType, LongType) - 0) as double) / cast(86400000000 as double))) as double) = (cast((precisetimestampconversion(InvoiceDate#1278, TimestampType, LongType) - 0) as double) / cast(86400000000 as double))) THEN (CEIL((cast((precisetimestampconversion(InvoiceDate#1278, TimestampType, LongType) - 0) as double) / cast(86400000000 as double))) + cast(1 as bigint)) ELSE CEIL((cast((precisetimestampconversion(InvoiceDate#1278, TimestampType, LongType) - 0) as double) / cast(86400000000 as double))) END + cast(0 as bigint)) - cast(1 as bigint)) * 86400000000) + 0) + 86400000000), LongType, TimestampType)) AS window#2722, InvoiceNo#1194, StockCode#1195, Description#1196, Quantity#1197, invoiceDate#1278, UnitPrice#1199, CustomerID#1200, Country#1201, Amount#2515]\n               +- Project [InvoiceNo#1194, StockCode#1195, Description#1196, Quantity#1197, invoiceDate#1278, UnitPrice#1199, CustomerID#1200, Country#1201, (cast(Quantity#1197 as double) * UnitPrice#1199) AS Amount#2515]\n                  +- Filter (Quantity#1197 > 0)\n                     +- Project [InvoiceNo#1194, StockCode#1195, Description#1196, Quantity#1197, to_timestamp(InvoiceDate#1198, Some(MM/dd/yy H:mm)) AS invoiceDate#1278, UnitPrice#1199, CustomerID#1200, Country#1201]\n                        +- Relation[InvoiceNo#1194,StockCode#1195,Description#1196,Quantity#1197,InvoiceDate#1198,UnitPrice#1199,CustomerID#1200,Country#1201] csv\n\nset zeppelin.spark.sql.stacktrace = true to see full stacktrace"}]},"apps":[],"jobName":"paragraph_1570548466408_1727838906","id":"20190520-140933_785400989","dateCreated":"2019-10-08T15:27:46+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:201","dateFinished":"2019-10-15T18:53:12+0000","dateStarted":"2019-10-15T18:53:12+0000"},{"text":"","user":"anonymous","dateUpdated":"2019-10-08T15:27:46+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------------------+------------------+\n|              start|       sum(amount)|\n+-------------------+------------------+\n|2010-11-24 19:00:00| 58833.88000000002|\n|2010-12-01 19:00:00|         266320.76|\n|2010-12-08 19:00:00|234844.27999999997|\n|2010-12-15 19:00:00|177360.10999999993|\n|2010-12-22 19:00:00|11796.309999999992|\n+-------------------+------------------+\nonly showing top 5 rows\n\nweeklySalesDf: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [start: timestamp, sum(amount): double]\n"}]},"apps":[],"jobName":"paragraph_1570548466409_115703556","id":"20190520-140933_428817963","dateCreated":"2019-10-08T15:27:46+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:202"},{"text":"%sql\n","user":"anonymous","dateUpdated":"2019-10-15T17:30:02+0000","config":{"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/sql","fontSize":9,"results":{"0":{"graph":{"mode":"lineChart","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"to_date(weeklysales.`start`)":"string","sum(amount)":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false},"multiBarChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"},"lineChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"}},"commonSetting":{},"keys":[{"name":"to_date(weeklysales.`start`)","index":0,"aggr":"sum"}],"groups":[],"values":[{"name":"sum(amount)","index":1,"aggr":"sum"}]},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"to_date(weeklysales.`start`)\tsum(amount)\n2010-11-24\t58833.88000000002\n2010-12-01\t266320.76\n2010-12-08\t234844.27999999997\n2010-12-15\t177360.10999999993\n2010-12-22\t11796.309999999992\n2010-12-29\t13729.059999999998\n2011-01-05\t196304.22999999995\n2011-01-12\t149317.07000000027\n2011-01-19\t137741.20000000004\n2011-01-26\t118809.36000000002\n2011-02-02\t117429.22000000006\n2011-02-09\t128847.81000000004\n2011-02-16\t138099.13000000003\n2011-02-23\t116085.18999999996\n2011-03-02\t147615.44000000015\n2011-03-09\t122114.17999999996\n2011-03-16\t150047.39000000004\n2011-03-23\t219388.9\n2011-03-30\t132980.16000000003\n2011-04-06\t123609.12000000005\n2011-04-13\t161846.1409999998\n2011-04-20\t87374.70000000004\n2011-04-27\t75286.71999999997\n2011-05-04\t189260.63000000006\n2011-05-11\t215677.61000000013\n2011-05-18\t176879.23999999993\n2011-05-25\t110808.24000000008\n2011-06-01\t172149.95999999996\n2011-06-08\t187264.3100000001\n2011-06-15\t155310.8099999999\n2011-06-22\t112595.35999999997\n2011-06-29\t173752.81\n2011-07-06\t123823.4900000001\n2011-07-13\t169295.00999999995\n2011-07-20\t152603.38100000005\n2011-07-27\t171116.80000000002\n2011-08-03\t163875.63999999987\n2011-08-10\t160414.12000000002\n2011-08-17\t188864.5300000001\n2011-08-24\t106552.79000000001\n2011-08-31\t195082.47000000012\n2011-09-07\t198719.58\n2011-09-14\t306781.53000000014\n2011-09-21\t235491.70199999987\n2011-09-28\t286780.8100000002\n2011-10-05\t226603.03000000014\n2011-10-12\t222388.63000000003\n2011-10-19\t253114.09999999998\n2011-10-26\t247555.6999999997\n2011-11-02\t355058.8800000002\n2011-11-09\t394675.79999999993\n2011-11-16\t332354.05000000016\n2011-11-23\t306160.31\n2011-11-30\t320118.69999999984\n2011-12-07\t113549.30999999985\n"},{"type":"TEXT","data":""}]},"apps":[],"jobName":"paragraph_1570548466410_-547936755","id":"20190520-212256_1274740776","dateCreated":"2019-10-08T15:27:46+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:203"}],"name":"Jarvis/2-DataFrame_milad","id":"2ESP6R6J2","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}