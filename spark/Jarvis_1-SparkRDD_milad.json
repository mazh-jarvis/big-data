{"paragraphs":[{"text":"%toc","user":"anonymous","dateUpdated":"2019-10-01T14:18:28+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1569714209010_-1936813872","id":"20190928-234329_664355117","dateCreated":"2019-09-28T23:43:29+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:502"},{"text":"%md\n\n# Learning Spark\nApache Spark is an open-source distributed general-purpose cluster-computing framework. Spark provides an interface for programming entire clusters with implicit data parallelism and fault tolerance. Originally developed at the University of California, Berkeley's AMPLab, the Spark codebase was later donated to the Apache Software Foundation, which has maintained it since. [wiki](https://en.wikipedia.org/wiki/Apache_Spark)\n\n## Learning Objective\nWe will first learn single node RDD like another data structure. We will then introduce the distributed mode and talk about distributed variables. Eventually, we will study how to use higher-level tools like Spark SQL and Dataframe.\n\n- Spark RDD\n    - Focus on RDD operations(transformations/actions) as a data structure\n- Spark Architecture and cluster mode\n    - Focus on Spark cluster mode\n    - Optimization\n- Spark SQL and Dataframe\n    - Familiar with high-level tools that replace RDD\n- Spark structured streaming\n\n## Learning Spark RDD\n\nIn this section, we will only focus on single node RDD like another data structure. We will learn how to create and manipulate RDDs without worrying how does it work in a distributed system. In the next section, we will study the Spark architecture and different deploy modes.\n\n**Readings**\n\n- `Spark: The Definitive Guide: Big Data Processing Made Simple` (`SDG`) Chapter 12 & 13\n- Spark RDD Doc: https://spark.apache.org/docs/2.3.3/rdd-programming-guide.html\n- RDD paper [link](https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf)\n\nPlease try to learn Spark and Scala using **Scala Docs**\n\n- Scala: https://www.scala-lang.org/api/2.11.8/#package\n- Spark Scala Doc: https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.RDD\n\n**Definition**\nAn RDD is a resilient and distributed collection of records spread over one or many partitions.\n\n- Resilient, i.e. fault-tolerant with the help of RDD lineage graph and so able to recompute missing or damaged partitions due to node failures.\n- Distributed with data residing on multiple nodes in a cluster.\n- Dataset is a collection of partitioned data with primitive values or values of values, e.g. tuples or other objects (that represent records of the data you work with).\n\n**Creating RDDs**\n\n```scala\n//create an RDD from a exiting data structure\nval listRdd = SparkContext.parallelize(List(1,2,3)\n\n//create an RDD from data sources \nspark.sparkContext.textFile(\"/tmp/myTest.txt\")\nspark.sparkContext.textFile(\"hdfs:///tmp/myTest.txt\")\n```\n\n**Manipulating RDDs**\nRDDs support two kinds of operations:\n\n- transformations - lazy operations that return another RDD.\n- actions - operations that trigger computation and return values.","user":"anonymous","dateUpdated":"2019-10-01T14:18:29+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Learning Spark</h1>\n<p>Apache Spark is an open-source distributed general-purpose cluster-computing framework. Spark provides an interface for programming entire clusters with implicit data parallelism and fault tolerance. Originally developed at the University of California, Berkeley&rsquo;s AMPLab, the Spark codebase was later donated to the Apache Software Foundation, which has maintained it since. <a href=\"https://en.wikipedia.org/wiki/Apache_Spark\">wiki</a></p>\n<h2>Learning Objective</h2>\n<p>We will first learn single node RDD like another data structure. We will then introduce the distributed mode and talk about distributed variables. Eventually, we will study how to use higher-level tools like Spark SQL and Dataframe.</p>\n<ul>\n  <li>Spark RDD\n    <ul>\n      <li>Focus on RDD operations(transformations/actions) as a data structure</li>\n    </ul>\n  </li>\n  <li>Spark Architecture and cluster mode\n    <ul>\n      <li>Focus on Spark cluster mode</li>\n      <li>Optimization</li>\n    </ul>\n  </li>\n  <li>Spark SQL and Dataframe\n    <ul>\n      <li>Familiar with high-level tools that replace RDD</li>\n    </ul>\n  </li>\n  <li>Spark structured streaming</li>\n</ul>\n<h2>Learning Spark RDD</h2>\n<p>In this section, we will only focus on single node RDD like another data structure. We will learn how to create and manipulate RDDs without worrying how does it work in a distributed system. In the next section, we will study the Spark architecture and different deploy modes.</p>\n<p><strong>Readings</strong></p>\n<ul>\n  <li><code>Spark: The Definitive Guide: Big Data Processing Made Simple</code> (<code>SDG</code>) Chapter 12 &amp; 13</li>\n  <li>Spark RDD Doc: <a href=\"https://spark.apache.org/docs/2.3.3/rdd-programming-guide.html\">https://spark.apache.org/docs/2.3.3/rdd-programming-guide.html</a></li>\n  <li>RDD paper <a href=\"https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf\">link</a></li>\n</ul>\n<p>Please try to learn Spark and Scala using <strong>Scala Docs</strong></p>\n<ul>\n  <li>Scala: <a href=\"https://www.scala-lang.org/api/2.11.8/#package\">https://www.scala-lang.org/api/2.11.8/#package</a></li>\n  <li>Spark Scala Doc: <a href=\"https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.RDD\">https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.RDD</a></li>\n</ul>\n<p><strong>Definition</strong><br/>An RDD is a resilient and distributed collection of records spread over one or many partitions.</p>\n<ul>\n  <li>Resilient, i.e. fault-tolerant with the help of RDD lineage graph and so able to recompute missing or damaged partitions due to node failures.</li>\n  <li>Distributed with data residing on multiple nodes in a cluster.</li>\n  <li>Dataset is a collection of partitioned data with primitive values or values of values, e.g. tuples or other objects (that represent records of the data you work with).</li>\n</ul>\n<p><strong>Creating RDDs</strong></p>\n<pre><code class=\"scala\">//create an RDD from a exiting data structure\nval listRdd = SparkContext.parallelize(List(1,2,3)\n\n//create an RDD from data sources \nspark.sparkContext.textFile(&quot;/tmp/myTest.txt&quot;)\nspark.sparkContext.textFile(&quot;hdfs:///tmp/myTest.txt&quot;)\n</code></pre>\n<p><strong>Manipulating RDDs</strong><br/>RDDs support two kinds of operations:</p>\n<ul>\n  <li>transformations - lazy operations that return another RDD.</li>\n  <li>actions - operations that trigger computation and return values.</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1569247436412_-421511120","id":"20190920-182115_2130024632","dateCreated":"2019-09-23T14:03:56+0000","dateStarted":"2019-10-01T14:18:29+0000","dateFinished":"2019-10-01T14:18:31+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:503"},{"text":"%md\n### How to interact with Spark\n\nTo start a Spark job (either single JVM or distributed mode), we can simply execute `bin/spark-shell` cmd which will launch JVM which has the Spark Job running. The entry point of the Spark Job (JVM) is called SparkSession which allows user/application interact with the Spark Job. For instance, you can create RDDs through SparkSession from a exiting Scala collection (e.g. Array, List, Set) or from a data source (e.g. read text from HDFS to RDD)\n\n> Compare SparkContext and Spark Sessionhttps://databricks.com/blog/2016/08/15/how-to-use-sparksession-in-apache-spark-2-0.html https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.sql.SparkSession https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.SparkContext\n\nAlternatively, we can use Zeppelin to create a Spark Job and Zeppline will automatically create a SparkSession (`spark`) and a SparkContext (`sc`) for you.\n","user":"anonymous","dateUpdated":"2019-10-01T14:18:32+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>How to interact with Spark</h3>\n<p>To start a Spark job (either single JVM or distributed mode), we can simply execute <code>bin/spark-shell</code> cmd which will launch JVM which has the Spark Job running. The entry point of the Spark Job (JVM) is called SparkSession which allows user/application interact with the Spark Job. For instance, you can create RDDs through SparkSession from a exiting Scala collection (e.g. Array, List, Set) or from a data source (e.g. read text from HDFS to RDD)</p>\n<blockquote>\n  <p>Compare SparkContext and Spark <a href=\"Sessionhttps://databricks.com/blog/2016/08/15/how-to-use-sparksession-in-apache-spark-2-0.html\">Sessionhttps://databricks.com/blog/2016/08/15/how-to-use-sparksession-in-apache-spark-2-0.html</a> <a href=\"https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.sql.SparkSession\">https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.sql.SparkSession</a> <a href=\"https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.SparkContext\">https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.SparkContext</a></p>\n</blockquote>\n<p>Alternatively, we can use Zeppelin to create a Spark Job and Zeppline will automatically create a SparkSession (<code>spark</code>) and a SparkContext (<code>sc</code>) for you.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1569247436423_1304406610","id":"20190921-014743_1530188134","dateCreated":"2019-09-23T14:03:56+0000","dateStarted":"2019-10-01T14:18:32+0000","dateFinished":"2019-10-01T14:18:32+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:504"},{"text":"%spark\n\n//Spark session and sparkContext are loaded automatically\nprintln(spark.version.to)\nprintln(spark)\n\n//The following two lines point to the same SparkContext@2a695829 where @2a695829 is the memory address\nprintln(spark.sparkContext)\nprintln(sc)\n","user":"anonymous","dateUpdated":"2019-10-01T14:18:32+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"2.3.3\norg.apache.spark.sql.SparkSession@245c7005\norg.apache.spark.SparkContext@5595def\norg.apache.spark.SparkContext@5595def\n"}]},"apps":[],"jobName":"paragraph_1569247436424_1811285544","id":"20190921-013657_404311467","dateCreated":"2019-09-23T14:03:56+0000","dateStarted":"2019-10-01T14:18:33+0000","dateFinished":"2019-10-01T14:18:35+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:505"},{"text":"%md","user":"anonymous","dateUpdated":"2019-10-01T14:18:35+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1569247436425_-1556500424","id":"20190922-220218_788870347","dateCreated":"2019-09-23T14:03:56+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:506"},{"text":"%md\n### Creating RDDs from Scala collections\n\nWe can use `sc.parallelize` method to create RDDs from Scala collections\n(Note: `parallelize` is not available in the `SparkSession`. However, you can use `SparkSession.sparkContext.parallelize` instead)\n\nhttps://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.SparkContext","user":"anonymous","dateUpdated":"2019-10-01T14:18:35+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Creating RDDs from Scala collections</h3>\n<p>We can use <code>sc.parallelize</code> method to create RDDs from Scala collections<br/>(Note: <code>parallelize</code> is not available in the <code>SparkSession</code>. However, you can use <code>SparkSession.sparkContext.parallelize</code> instead)</p>\n<p><a href=\"https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.SparkContext\">https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.SparkContext</a></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1569247436426_-1509700491","id":"20190921-022812_325072599","dateCreated":"2019-09-23T14:03:56+0000","dateStarted":"2019-10-01T14:18:36+0000","dateFinished":"2019-10-01T14:18:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:507"},{"text":"%spark\n//Create RDDs from Scala collections\nval lsRdd = sc.parallelize(List(1,2,3,4,5))\n\n//number of items in lsRdd\nval count = lsRdd.count\n\n//first element in lsRdd\nval firstE = lsRdd.first\n\n//Number of partitions\nval partitionsNum = lsRdd.partitions.length\n\n//Manipulating lsRDD\nval dupRdd = lsRdd.flatMap(i => List.fill(i)(i))\nval dupArray = dupRdd.collect\nval evens = dupRdd.filter(_%2 == 0).collect","user":"anonymous","dateUpdated":"2019-10-01T14:18:36+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"lsRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:27\ncount: Long = 5\nfirstE: Int = 1\npartitionsNum: Int = 2\ndupRdd: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[1] at flatMap at <console>:29\ndupArray: Array[Int] = Array(1, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 5)\nevens: Array[Int] = Array(2, 2, 4, 4, 4, 4)\n"}]},"apps":[],"jobName":"paragraph_1569247436427_-752320664","id":"20190921-020350_225494359","dateCreated":"2019-09-23T14:03:56+0000","dateStarted":"2019-10-01T14:18:36+0000","dateFinished":"2019-10-01T14:18:42+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:508"},{"text":"%md","user":"anonymous","dateUpdated":"2019-10-01T14:18:42+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1569247436427_228670661","id":"20190922-220230_613999600","dateCreated":"2019-09-23T14:03:56+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:509"},{"text":"%md\n### Creating RDDs from Data source\n\n- `ssh` to dataproc master node\n- Download `online-retail-dataset.txt` dataset [link](https://github.com/databricks/Spark-The-Definitive-Guide/blob/master/data/retail-data/all/online-retail-dataset.csv)\n- Upload `online-retail-dataset.txt` to a HDFS location (e.g. hdfs dfs -put ...)\n- Inspect the dataset using spark RDD (see below Spark code)\n- Discuss why there are some lines that have more than 8 columns (hint: csv format)\n- Discuss some possible solutions ","user":"anonymous","dateUpdated":"2019-10-01T14:18:43+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Creating RDDs from Data source</h3>\n<ul>\n  <li><code>ssh</code> to dataproc master node</li>\n  <li>Download <code>online-retail-dataset.txt</code> dataset <a href=\"https://github.com/databricks/Spark-The-Definitive-Guide/blob/master/data/retail-data/all/online-retail-dataset.csv\">link</a></li>\n  <li>Upload <code>online-retail-dataset.txt</code> to a HDFS location (e.g. hdfs dfs -put &hellip;)</li>\n  <li>Inspect the dataset using spark RDD (see below Spark code)</li>\n  <li>Discuss why there are some lines that have more than 8 columns (hint: csv format)</li>\n  <li>Discuss some possible solutions</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1569247436428_850356150","id":"20190920-182511_1653833929","dateCreated":"2019-09-23T14:03:56+0000","dateStarted":"2019-10-01T14:18:44+0000","dateFinished":"2019-10-01T14:18:44+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:510"},{"text":"%spark\nval retailRDD = sc.textFile(\"hdfs:///user/milad/datasets/online_retail/online-retail-dataset.csv\")\n\n//count number of elements in the RDD\nretailRDD.count\n\n//understand what does each element look like in RDD\nval firstE = retailRDD.first()\n\n//find out does withReplacement mean from the scal doc https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.RDD\nretailRDD.takeSample(false, 10, 1).foreach(println)","user":"anonymous","dateUpdated":"2019-10-01T14:18:44+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"retailRDD: org.apache.spark.rdd.RDD[String] = hdfs:///user/milad/datasets/online_retail/online-retail-dataset.csv MapPartitionsRDD[4] at textFile at <console>:25\nres24: Long = 541910\nfirstE: String = InvoiceNo,StockCode,Description,Quantity,InvoiceDate,UnitPrice,CustomerID,Country\n552290,21430,SET/3 RED GINGHAM ROSE STORAGE BOX,1,5/8/2011 13:32,3.75,16007,United Kingdom\n578349,22636,CHILDS BREAKFAST SET CIRCUS PARADE,2,11/24/2011 9:50,8.5,14539,United Kingdom\n537666,84917,WHITE HAND TOWEL WITH BUTTERFLY,1,12/7/2010 18:36,4.21,,United Kingdom\n547021,20749,ASSORTED COLOUR MINI CASES,2,3/18/2011 15:43,7.95,13046,United Kingdom\n553718,35809A,ENAMEL PINK TEA CONTAINER,1,5/18/2011 16:14,2.46,,United Kingdom\n557466,21242,RED RETROSPOT PLATE ,8,6/20/2011 13:08,1.69,13815,Germany\n567160,21218,RED SPOTTY BISCUIT TIN,1,9/18/2011 10:35,3.75,14562,United Kingdom\n548975,17003,BROCADE RING PURSE ,108,4/5/2011 11:47,0.29,17596,United Kingdom\n559338,85086A,CANDY SPOT HEART DECORATION,1,7/7/2011 16:30,0.83,,United Kingdom\n546769,22499,WOODEN UNION JACK BUNTING,3,3/16/2011 14:57,5.95,17504,United Kingdom\n"}]},"apps":[],"jobName":"paragraph_1569247436429_-119772419","id":"20190920-182724_1961848616","dateCreated":"2019-09-23T14:03:56+0000","dateStarted":"2019-10-01T14:18:44+0000","dateFinished":"2019-10-01T14:18:50+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:511"},{"text":"%md\n","user":"anonymous","dateUpdated":"2019-10-01T14:18:50+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1569247436429_608725797","id":"20190922-220256_1973670371","dateCreated":"2019-09-23T14:03:56+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:512"},{"text":"%md\n### CSV format issue\n- Discuss why there are some lines that have more than 8 columns (hint: csv format)\n- Discuss some possible solutions","user":"anonymous","dateUpdated":"2019-10-01T14:18:51+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>CSV format issue</h3>\n<ul>\n  <li>Discuss why there are some lines that have more than 8 columns (hint: csv format)</li>\n  <li>Discuss some possible solutions</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1569247436430_-1069709751","id":"20190921-023538_989684097","dateCreated":"2019-09-23T14:03:56+0000","dateStarted":"2019-10-01T14:18:51+0000","dateFinished":"2019-10-01T14:18:51+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:513"},{"text":"%spark\n//Check CSV format\nval splitRdd = retailRDD.map(s => s.split(\",\"))\n\n//Some lines have more than 8 column which indicates a format issue\nval samples = splitRdd.map(arr => arr.length).takeSample(false,15, 11)\n\n//find out how many lines have more than 8 cols\nval lenArrRdd = splitRdd.map(arr => (arr.length, arr))\nlenArrRdd.filter(_._1 != 8).take(3).foreach({case(count, cols) => println(count + \":\" + cols.mkString(\"||\"))})\n","user":"anonymous","dateUpdated":"2019-10-01T14:18:51+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"splitRdd: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[6] at map at <console>:29\nsamples: Array[Int] = Array(8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8)\nlenArrRdd: org.apache.spark.rdd.RDD[(Int, Array[String])] = MapPartitionsRDD[9] at map at <console>:31\n9:536381||82567||\"AIRLINE LOUNGE||METAL SIGN\"||2||12/1/2010 9:41||2.1||15311||United Kingdom\n9:536394||21506||\"FANCY FONT BIRTHDAY CARD|| \"||24||12/1/2010 10:39||0.42||13408||United Kingdom\n9:536520||22760||\"TRAY|| BREAKFAST IN BED\"||1||12/1/2010 12:43||12.75||14729||United Kingdom\n"}]},"apps":[],"jobName":"paragraph_1569247436430_-1909340861","id":"20190921-023311_1509488233","dateCreated":"2019-09-23T14:03:56+0000","dateStarted":"2019-10-01T14:18:52+0000","dateFinished":"2019-10-01T14:18:55+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:514"},{"text":"%md\n### Pre-process dataset\n\nWe need to deal with fields that contain commas. e.g. `123,\"seond, field\",\"third field\"`. We have seen this csv format issue in Hive, and we solved it using `OpenCSV` SerDe. In this practice, we will simply remove commas between double quotes and then remove all double quotes.\n\n- Removing commas in `Description` field (e.g. \"Apple, Inc\" => \"Apple Inc\")<br>`awk -F'\"' -v OFS='' '{ for (i=2; i<=NF; i+=2) gsub(\",\", \"\", $i) } 1' online-retail-dataset.txt`\n- Remove all double double quotes<br>`sed 's/\"//g' online-retail-dataset.txt`\n- output file: `online-retail-dataset_clean.txt`\n\n### Move file to HDFS\nMove `online-retail-dataset_clean.txt` to GCP Dataproc HDFS\n\n### Spark RDD Cache\n- https://spark.apache.org/docs/2.3.3/rdd-programming-guide.html#rdd-persistence\n- https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.RDD@cache():RDD.this.type","user":"anonymous","dateUpdated":"2019-10-01T14:18:55+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Pre-process dataset</h3>\n<p>We need to deal with fields that contain commas. e.g. <code>123,&quot;seond, field&quot;,&quot;third field&quot;</code>. We have seen this csv format issue in Hive, and we solved it using <code>OpenCSV</code> SerDe. In this practice, we will simply remove commas between double quotes and then remove all double quotes.</p>\n<ul>\n  <li>Removing commas in <code>Description</code> field (e.g. &ldquo;Apple, Inc&rdquo; =&gt; &ldquo;Apple Inc&rdquo;)<br><code>awk -F&#39;&quot;&#39; -v OFS=&#39;&#39; &#39;{ for (i=2; i&lt;=NF; i+=2) gsub(&quot;,&quot;, &quot;&quot;, $i) } 1&#39; online-retail-dataset.txt</code></li>\n  <li>Remove all double double quotes<br><code>sed &#39;s/&quot;//g&#39; online-retail-dataset.txt</code></li>\n  <li>output file: <code>online-retail-dataset_clean.txt</code></li>\n</ul>\n<h3>Move file to HDFS</h3>\n<p>Move <code>online-retail-dataset_clean.txt</code> to GCP Dataproc HDFS</p>\n<h3>Spark RDD Cache</h3>\n<ul>\n  <li><a href=\"https://spark.apache.org/docs/2.3.3/rdd-programming-guide.html#rdd-persistence\">https://spark.apache.org/docs/2.3.3/rdd-programming-guide.html#rdd-persistence</a></li>\n  <li><a href=\"https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.RDD@cache():RDD.this.type\">https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.RDD@cache():RDD.this.type</a></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1569247436431_1915141596","id":"20190519-113048_765206384","dateCreated":"2019-09-23T14:03:56+0000","dateStarted":"2019-10-01T14:18:56+0000","dateFinished":"2019-10-01T14:18:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:515"},{"text":"//Load csv file\n//Lazy evaluation\n//val datasetDir = \"/home/centos/dev/jrvs/bootcamp/hadoop/datasets\"\nval filePath = \"hdfs:///user/milad/datasets/online_retail/online-retail-dataset_clean.txt\"\nval retailRDDHeader = sc.textFile(filePath)\nval header = retailRDDHeader.first\nval retailRDD = retailRDDHeader.filter(r => r!=header)\n\n//RDD action triggers evaluation (in this case count is an action)\nval count = retailRDD.count\n\n//tip: Use tab key to auto-complete\nval sample3 = retailRDD.takeSample(false, 3, 22)\n\n//Make sure every row has exactly 0 columns\nval longRow = retailRDD.filter(row => row.split(\",\").length != 8 ).count\n\n//Cache RDD since it will be accessed frequently\nretailRDD.cache","user":"anonymous","dateUpdated":"2019-10-03T16:48:29+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"filePath: String = hdfs:///user/milad/datasets/online_retail/online-retail-dataset_clean.txt\nretailRDDHeader: org.apache.spark.rdd.RDD[String] = hdfs:///user/milad/datasets/online_retail/online-retail-dataset_clean.txt MapPartitionsRDD[1] at textFile at <console>:27\nheader: String = InvoiceNo,StockCode,Description,Quantity,InvoiceDate,UnitPrice,CustomerID,Country\nretailRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at filter at <console>:31\ncount: Long = 541909\nsample3: Array[String] = Array(569457,22329,ROUND CONTAINER SET OF 5 RETROSPOT,1,10/4/2011 11:29,1.65,14606,United Kingdom, 571265,22530,MAGIC DRAWING SLATE DOLLY GIRL ,2,10/16/2011 11:31,0.42,16674,United Kingdom, 563893,90064B,BLACK VINTAGE  CRYSTAL EARRINGS,1,8/19/2011 17:10,3.75,16330,United Kingdom)\nlongRow: Long = 0\nres23: retailRDD.type = MapPartitionsRDD[2] at filter at <console>:31\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://mazh-jrvs-bootcamp-m.us-east1-b.c.rich-suprstate-244018.internal:4040/jobs/job?id=0","http://mazh-jrvs-bootcamp-m.us-east1-b.c.rich-suprstate-244018.internal:4040/jobs/job?id=1","http://mazh-jrvs-bootcamp-m.us-east1-b.c.rich-suprstate-244018.internal:4040/jobs/job?id=2","http://mazh-jrvs-bootcamp-m.us-east1-b.c.rich-suprstate-244018.internal:4040/jobs/job?id=3","http://mazh-jrvs-bootcamp-m.us-east1-b.c.rich-suprstate-244018.internal:4040/jobs/job?id=4"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1569247436432_-1952106025","id":"20190519-105016_1691323616","dateCreated":"2019-09-23T14:03:56+0000","dateStarted":"2019-10-03T16:48:29+0000","dateFinished":"2019-10-03T16:48:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:516"},{"text":"//Making some utilities and make your life easier :)\n//val printRddNSamples = (rdd: org.apache.spark.rdd.RDD[_], n: Int) => rdd.takeSample(false, n, 22).foreach(println)\n//val printRdd3Samples = (rdd: org.apache.spark.rdd.RDD[_]) => printRddNSamples(rdd, 3)\n//val printRddTopN = (rdd: org.apache.spark.rdd.RDD[_], n: Int) => rdd.take(n).foreach(println)\nval bars = \"---------\"\nval printMsg = (msg:String) => println(bars+msg+bars)\n","user":"anonymous","dateUpdated":"2019-10-01T14:19:01+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"bars: String = ---------\nprintMsg: String => Unit = <function1>\n"}]},"apps":[],"jobName":"paragraph_1569247436432_1445649189","id":"20190519-192640_1954412488","dateCreated":"2019-09-23T14:03:56+0000","dateStarted":"2019-10-01T14:19:01+0000","dateFinished":"2019-10-01T14:19:02+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:517"},{"text":"%md\n### Spark RDD Transormations and Actions\n\n- <a href=http://spark.apache.org/docs/2.3.0/rdd-programming-guide.html#transformations target=\"_blank\">Transformations</a>\n- <a href=http://spark.apache.org/docs/2.3.0/rdd-programming-guide.html#actions target=\"_blank\">Actions</a>\n- Databrick RDD operations http://bit.ly/30ez9IG\n- `SDG` chapter 12 & 13\n\n#### RDD Actions\n1. Get the first element from `retailRDD`\n2. Get the first 5 elements from `retailRDD` as an array.\n3. Get all elements from `retailRDD` as an array\n4. Get random 5 elements from `retailRDD` as an array\n5. Save all elements from `retailRDD` to local file `hdfs:///tmp/text.txt`\n\nSample outputs:\n```bash\n#1\n536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,12/1/2010 8:26,2.55,17850,United Kingdom\n#2\n536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,12/1/2010 8:26,2.55,17850,United Kingdom\n536365,71053,WHITE METAL LANTERN,6,12/1/2010 8:26,3.39,17850,United Kingdom\n536365,84406B,CREAM CUPID HEARTS COAT HANGER,8,12/1/2010 8:26,2.75,17850,United Kingdom\n536365,84029G,KNITTED UNION FLAG HOT WATER BOTTLE,6,12/1/2010 8:26,3.39,17850,United Kingdom\n536365,84029E,RED WOOLLY HOTTIE WHITE HEART.,6,12/1/2010 8:26,3.39,17850,United Kingdom\n#3\n541909\n#4\n575477,85099F,JUMBO BAG STRAWBERRY,2,11/9/2011 16:14,4.13,,United Kingdom\n574536,22988,SOLDIERS EGG CUP ,5,11/4/2011 14:35,1.25,15707,United Kingdom\n538104,22748,POPPYS PLAYHOUSE KITCHEN,2,12/9/2010 15:16,2.1,17950,United Kingdom\n555150,84991,60 TEATIME FAIRY CAKE CASES,24,5/31/2011 15:53,0.55,,United Kingdom\n550496,22497,SET OF 2 TINS VINTAGE BATHROOM ,1,4/18/2011 15:05,8.29,,United Kingdom\n#5\nedward@jarvis-hadoop-m:~/BigData$ hdfs dfs -ls /tmp\ndrwxr-xr-x   - zeppelin hadoop          0 2019-09-17 18:20 /tmp/test.txt\n\n```\n\n","user":"anonymous","dateUpdated":"2019-10-01T14:19:02+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Spark RDD Transormations and Actions</h3>\n<ul>\n  <li>\n  <a href=http://spark.apache.org/docs/2.3.0/rdd-programming-guide.html#transformations target=\"_blank\">Transformations</a></li>\n  <li>\n  <a href=http://spark.apache.org/docs/2.3.0/rdd-programming-guide.html#actions target=\"_blank\">Actions</a></li>\n  <li>Databrick RDD operations <a href=\"http://bit.ly/30ez9IG\">http://bit.ly/30ez9IG</a></li>\n  <li><code>SDG</code> chapter 12 &amp; 13</li>\n</ul>\n<h4>RDD Actions</h4>\n<ol>\n  <li>Get the first element from <code>retailRDD</code></li>\n  <li>Get the first 5 elements from <code>retailRDD</code> as an array.</li>\n  <li>Get all elements from <code>retailRDD</code> as an array</li>\n  <li>Get random 5 elements from <code>retailRDD</code> as an array</li>\n  <li>Save all elements from <code>retailRDD</code> to local file <code>hdfs:///tmp/text.txt</code></li>\n</ol>\n<p>Sample outputs:</p>\n<pre><code class=\"bash\">#1\n536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,12/1/2010 8:26,2.55,17850,United Kingdom\n#2\n536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,12/1/2010 8:26,2.55,17850,United Kingdom\n536365,71053,WHITE METAL LANTERN,6,12/1/2010 8:26,3.39,17850,United Kingdom\n536365,84406B,CREAM CUPID HEARTS COAT HANGER,8,12/1/2010 8:26,2.75,17850,United Kingdom\n536365,84029G,KNITTED UNION FLAG HOT WATER BOTTLE,6,12/1/2010 8:26,3.39,17850,United Kingdom\n536365,84029E,RED WOOLLY HOTTIE WHITE HEART.,6,12/1/2010 8:26,3.39,17850,United Kingdom\n#3\n541909\n#4\n575477,85099F,JUMBO BAG STRAWBERRY,2,11/9/2011 16:14,4.13,,United Kingdom\n574536,22988,SOLDIERS EGG CUP ,5,11/4/2011 14:35,1.25,15707,United Kingdom\n538104,22748,POPPYS PLAYHOUSE KITCHEN,2,12/9/2010 15:16,2.1,17950,United Kingdom\n555150,84991,60 TEATIME FAIRY CAKE CASES,24,5/31/2011 15:53,0.55,,United Kingdom\n550496,22497,SET OF 2 TINS VINTAGE BATHROOM ,1,4/18/2011 15:05,8.29,,United Kingdom\n#5\nedward@jarvis-hadoop-m:~/BigData$ hdfs dfs -ls /tmp\ndrwxr-xr-x   - zeppelin hadoop          0 2019-09-17 18:20 /tmp/test.txt\n\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1569247436433_1888167790","id":"20190519-115905_2023471169","dateCreated":"2019-09-23T14:03:56+0000","dateStarted":"2019-10-01T14:19:03+0000","dateFinished":"2019-10-01T14:19:03+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:518"},{"text":"//your solution\n//you can ignore the output in this section or use it as hints\nprintMsg(\"#1\")\nretailRDD.first\n\nprintMsg(\"#2\")\nretailRDD take(6) foreach(println)\n\nprintMsg(\"#3\")\nretailRDD\n\nprintMsg(\"#4\")\nretailRDD takeSample(false, 6, System.nanoTime.toInt) foreach(println)\n\nprintMsg(\"#5\")\n//retailRDD saveAsTextFile(\"hdfs:///tmp/text.txt\")","user":"anonymous","dateUpdated":"2019-10-01T14:19:03+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{"0":{"graph":{"mode":"table","height":402.246,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"---------#1---------\nres51: String = 536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,12/1/2010 8:26,2.55,17850,United Kingdom\n---------#2---------\n536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,12/1/2010 8:26,2.55,17850,United Kingdom\n536365,71053,WHITE METAL LANTERN,6,12/1/2010 8:26,3.39,17850,United Kingdom\n536365,84406B,CREAM CUPID HEARTS COAT HANGER,8,12/1/2010 8:26,2.75,17850,United Kingdom\n536365,84029G,KNITTED UNION FLAG HOT WATER BOTTLE,6,12/1/2010 8:26,3.39,17850,United Kingdom\n536365,84029E,RED WOOLLY HOTTIE WHITE HEART.,6,12/1/2010 8:26,3.39,17850,United Kingdom\n536365,22752,SET 7 BABUSHKA NESTING BOXES,2,12/1/2010 8:26,7.65,17850,United Kingdom\n---------#3---------\nres55: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[13] at filter at <console>:31\n---------#4---------\n570404,21429,RED GINGHAM ROSE JEWELLERY BOX,4,10/10/2011 12:50,1.95,16395,United Kingdom\n555368,23091,ZINC HERB GARDEN CONTAINER,3,6/2/2011 14:30,6.25,13437,United Kingdom\n573393,22579,WOODEN TREE CHRISTMAS SCANDINAVIAN,24,10/30/2011 15:08,0.29,14158,United Kingdom\n550626,37489D,PINK/GREEN FLOWER DESIGN BIG MUG,4,4/19/2011 14:23,0.39,13184,United Kingdom\n576327,23505,PLAYING CARDS I LOVE LONDON ,16,11/14/2011 15:19,1.25,12476,Germany\n567340,21164,HOME SWEET HOME METAL SIGN ,6,9/19/2011 15:02,2.95,12540,Spain\n---------#5---------\n"}]},"apps":[],"jobName":"paragraph_1569247436434_59911122","id":"20190519-122034_629713430","dateCreated":"2019-09-23T14:03:56+0000","dateStarted":"2019-10-01T14:19:03+0000","dateFinished":"2019-10-01T14:19:07+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:519"},{"text":"%sh\nhdfs dfs -ls /tmp","user":"anonymous","dateUpdated":"2019-10-01T14:19:07+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Found 3 items\ndrwxrwxrwt   - hdfs     hadoop          0 2019-08-20 16:19 /tmp/hadoop-yarn\ndrwx-wx-wx   - hive     hadoop          0 2019-08-21 15:40 /tmp/hive\ndrwxr-xr-x   - zeppelin hadoop          0 2019-09-27 15:52 /tmp/text.txt\n"}]},"apps":[],"jobName":"paragraph_1569247436434_1874169086","id":"20190922-215221_1578966852","dateCreated":"2019-09-23T14:03:56+0000","dateStarted":"2019-10-01T14:19:08+0000","dateFinished":"2019-10-01T14:19:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:520"},{"text":"%md\n#### RDD Transformations\nRDD Transformations are lazy eval. You can trigger and verify you RDD with a action (e.g. `printRddNSamples` uses `takeSample` action) \n\n1. Get all sales from \"United Kingdom\" (hint: use fileter)\n2. Compare `sample` and `takeSample`\n\nSampel outputs:\n```bash\n#1\n495478\n495478\n495478\n#2\nrdd.sample vs rdd.takeSample\n```","user":"anonymous","dateUpdated":"2019-10-01T14:19:12+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>RDD Transformations</h4>\n<p>RDD Transformations are lazy eval. You can trigger and verify you RDD with a action (e.g. <code>printRddNSamples</code> uses <code>takeSample</code> action) </p>\n<ol>\n  <li>Get all sales from &ldquo;United Kingdom&rdquo; (hint: use fileter)</li>\n  <li>Compare <code>sample</code> and <code>takeSample</code></li>\n</ol>\n<p>Sampel outputs:</p>\n<pre><code class=\"bash\">#1\n495478\n495478\n495478\n#2\nrdd.sample vs rdd.takeSample\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1569247436435_-337210119","id":"20190917-181850_863623231","dateCreated":"2019-09-23T14:03:56+0000","dateStarted":"2019-10-01T14:19:12+0000","dateFinished":"2019-10-01T14:19:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:521"},{"text":"//your solution\n//you can ignore the output in this section or use it as hints\nval printRddNSamples = (rdd: org.apache.spark.rdd.RDD[_], n: Int) => rdd.takeSample(false, n, 22).foreach(println)\nprintMsg(\"#1\")\nval splitRdd = retailRDD.map(_.split(\",\"))\nval UKSamples = splitRdd.filter(_(7).equals(\"United Kingdom\"))\nprintRddNSamples(UKSamples.map(_(0)), 3)\nprintMsg(\"#2\")\nprintln(\"Method sample() takes a fraction of the RDD size, whereas takeSample() take the exact sample size\")","user":"anonymous","dateUpdated":"2019-10-01T14:19:12+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"printRddNSamples: (org.apache.spark.rdd.RDD[_], Int) => Unit = <function2>\n---------#1---------\nsplitRdd: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[17] at map at <console>:33\nUKSamples: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[18] at filter at <console>:35\n569532\n571442\n563935\n---------#2---------\nMethod sample() takes a fraction of the RDD size, whereas takeSample() take the exact sample size\n"}]},"apps":[],"jobName":"paragraph_1569247436435_-1445592152","id":"20190519-124053_1683164197","dateCreated":"2019-09-23T14:03:56+0000","dateStarted":"2019-10-01T14:19:13+0000","dateFinished":"2019-10-01T14:19:16+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:522"},{"text":"%md\n### Pair RDD (KeyValue)\nSo far, each element in `retailRDD` is a String. However, many distributed computation paradigms require KV pairs (e.g. MapReduce). Let's construct Pair RDDs from `retailRDD` in order to perform more advanced computations.\n\n**RDD vs PairRDD**:\n\n- https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.RDD\n- https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions","user":"anonymous","dateUpdated":"2019-10-01T14:19:16+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Pair RDD (KeyValue)</h3>\n<p>So far, each element in <code>retailRDD</code> is a String. However, many distributed computation paradigms require KV pairs (e.g. MapReduce). Let&rsquo;s construct Pair RDDs from <code>retailRDD</code> in order to perform more advanced computations.</p>\n<p><strong>RDD vs PairRDD</strong>:</p>\n<ul>\n  <li><a href=\"https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.RDD\">https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.RDD</a></li>\n  <li><a href=\"https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions\">https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions</a></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1569247436436_-1640135619","id":"20190519-125017_38292448","dateCreated":"2019-09-23T14:03:56+0000","dateStarted":"2019-10-01T14:19:16+0000","dateFinished":"2019-10-01T14:19:16+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:523"},{"text":"%md\n\n#### Questions 1.0\n\nTrnasform each element in `retailRDD` to a key value pair (as a tuple) as following\n\n```\nkey = country\nvalue = amount (e.g. Quantity * UnitPrice)\n```\n\nhint: \n\n- use `rdd.map`\n- Use `row.split(\",\")` to tokenize the row\n- Cast quanitity to int while parsing the row\n- Cast price to double\n\n**Sample output**:\n\n```\n//resultRdd.takeSample(false, 3,3)\n//wehere (key=country, value=amount) for each tuple\nres405: Array[(String, Double)] = Array((Germany,15.0), (United Kingdom,8.26), (United Kingdom,4.13))\n```\n\n","user":"anonymous","dateUpdated":"2019-10-01T14:19:17+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Questions 1.0</h4>\n<p>Trnasform each element in <code>retailRDD</code> to a key value pair (as a tuple) as following</p>\n<pre><code>key = country\nvalue = amount (e.g. Quantity * UnitPrice)\n</code></pre>\n<p>hint: </p>\n<ul>\n  <li>use <code>rdd.map</code></li>\n  <li>Use <code>row.split(&quot;,&quot;)</code> to tokenize the row</li>\n  <li>Cast quanitity to int while parsing the row</li>\n  <li>Cast price to double</li>\n</ul>\n<p><strong>Sample output</strong>:</p>\n<pre><code>//resultRdd.takeSample(false, 3,3)\n//wehere (key=country, value=amount) for each tuple\nres405: Array[(String, Double)] = Array((Germany,15.0), (United Kingdom,8.26), (United Kingdom,4.13))\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1569247436437_1090401319","id":"20190519-195132_1947538683","dateCreated":"2019-09-23T14:03:56+0000","dateStarted":"2019-10-01T14:19:17+0000","dateFinished":"2019-10-01T14:19:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:524"},{"text":"//Qustion 1.0 soutuion\nval parseKeyValue = (row: String ) => {\n    val tokens = row.split(\",\")\n    val country= tokens.last\n    val quantity = tokens(3)\n    val unitPrice = tokens(5)\n    val amount = quantity.toInt * unitPrice.toDouble\n    (country, amount)\n}\n\n\nval ctryRdd = retailRDD.map(parseKeyValue)\nctryRdd.takeSample(false, 3,3)","user":"anonymous","dateUpdated":"2019-09-30T19:28:05+0000","config":{"lineNumbers":true,"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{"0":{"graph":{"mode":"table","height":822,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"parseKeyValue: String => (String, Double) = <function1>\nctryRdd: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[99] at map at <console>:35\nres255: Array[(String, Double)] = Array((Germany,19.799999999999997), (United Kingdom,8.26), (United Kingdom,4.13))\n"}]},"apps":[],"jobName":"paragraph_1569247436437_936293626","id":"20190519-125921_348001552","dateCreated":"2019-09-23T14:03:56+0000","dateStarted":"2019-09-30T19:28:05+0000","dateFinished":"2019-09-30T19:28:07+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:525"},{"user":"anonymous","dateUpdated":"2019-09-23T14:03:56+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1569247436438_-1569287039","id":"20190922-215540_2030793994","dateCreated":"2019-09-23T14:03:56+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:526"},{"text":"%md\n\n#### Questions 1.1\n\nCalculate total sales amount for each country and sort in descending order\n\n```sql\nSELECT country, sum(Quantity * UnitPrice) as total_sales\nFROM retail\nGROUP BY country\nORDER BY total_sales\n```\n\n**Sample output**\n\n```bash\n//resultRdd.take(3)\n//key=country, value=total_sales\nres415: Array[(String, Double)] = Array((France,197403.90000000026), (Malta,2505.470000000001), (Greece,4710.5199999999995))\n```\n\nHints:\n\n- implement `group by` with `rdd.reduceByKey` [doc](http://bit.ly/30fJHHs)\n- implement `order by` with `rdd.sortBy`","user":"anonymous","dateUpdated":"2019-09-23T14:03:56+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Questions 1.1</h4>\n<p>Calculate total sales amount for each country and sort in descending order</p>\n<pre><code class=\"sql\">SELECT country, sum(Quantity * UnitPrice) as total_sales\nFROM retail\nGROUP BY country\nORDER BY total_sales\n</code></pre>\n<p><strong>Sample output</strong></p>\n<pre><code class=\"bash\">//resultRdd.take(3)\n//key=country, value=total_sales\nres415: Array[(String, Double)] = Array((France,197403.90000000026), (Malta,2505.470000000001), (Greece,4710.5199999999995))\n</code></pre>\n<p>Hints:</p>\n<ul>\n  <li>implement <code>group by</code> with <code>rdd.reduceByKey</code> <a href=\"http://bit.ly/30fJHHs\">doc</a></li>\n  <li>implement <code>order by</code> with <code>rdd.sortBy</code></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1569247436438_1885782761","id":"20190519-195238_1235609517","dateCreated":"2019-09-23T14:03:56+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:527"},{"text":"%spark\n//your solution\n//you can ignore the output in this section or use it as hints\nval salesByCtryRdd = ctryRdd.reduceByKey(_+_)\nprintRddNSamples(salesByCtryRdd, 4)\n\nval orderByCtryRdd = salesByCtryRdd.sortBy(_._2, ascending=false).take(4)\n//printRddNSamples(orderByCtryRdd, 4)","user":"anonymous","dateUpdated":"2019-10-02T00:16:49+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"salesByCtryRdd: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[93] at reduceByKey at <console>:42\n(Greece,2008584)\n(USA,3672086)\n(European Community,921588)\n(France,107648864)\norderByCtryRdd: Array[(String, Int)] = Array((United Kingdom,1331465299), (Germany,120075093), (EIRE,110391745), (France,107648864))\n"}]},"apps":[],"jobName":"paragraph_1569247436439_-1705707635","id":"20190519-195236_1582695577","dateCreated":"2019-09-23T14:03:56+0000","dateStarted":"2019-10-02T00:16:49+0000","dateFinished":"2019-10-02T00:16:51+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:528"},{"user":"anonymous","dateUpdated":"2019-09-23T14:03:56+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1569247436439_-454317591","id":"20190922-215553_1555963294","dateCreated":"2019-09-23T14:03:56+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:529"},{"text":"%md\n\n#### Questions 2.0\n\nLet's assume CustomerID is auto-increment. For each country, find the earliest registered customer (e.g. smallest CustomerID)\n\n```sql\nSELECT min(CustomerID), Country\nFROM retail\nGROUP BY Country\n```\n\n**Sample output**\n\n```\n//resultRdd.collect\nArray[(String, Int)] = Array((Australia,12386), (Portugal,12356), (United Kingdom,12346), (Brazil,12769), (Canada,15388), (Japan,12753), (Cyprus,12359), (European Community,15108), (Finland,12348), (Iceland,12347), (Netherlands,12759), (Singapore,12744), (Sweden,12483), (RSA,12446), (Norway,12350), (Denmark,12367), (Poland,12576), (Israel,12512), (Saudi Arabia,12565), (Belgium,12361), (Lithuania,15332), (Greece,12478), (Italy,12349), (France,12413), (Switzerland,12357), (Spain,12354), (USA,12558), (Germany,12426), (United Arab Emirates,12739), (EIRE,14016), (Hong Kong,2147483647), (Bahrain,12353), (Malta,15480), (Unspecified,12363), (Channel Islands,14442), (Austria,12358), (Lebanon,12764), (Czech Republic,12781))\n```\n\n**hints:**\n\n- Generate a new KV pair RDD `(country, id)`\n- use `rdd.reduceByKey` find the smallest\n- ID must be a numric number\n","user":"anonymous","dateUpdated":"2019-09-23T14:03:56+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Questions 2.0</h4>\n<p>Let&rsquo;s assume CustomerID is auto-increment. For each country, find the earliest registered customer (e.g. smallest CustomerID)</p>\n<pre><code class=\"sql\">SELECT min(CustomerID), Country\nFROM retail\nGROUP BY Country\n</code></pre>\n<p><strong>Sample output</strong></p>\n<pre><code>//resultRdd.collect\nArray[(String, Int)] = Array((Australia,12386), (Portugal,12356), (United Kingdom,12346), (Brazil,12769), (Canada,15388), (Japan,12753), (Cyprus,12359), (European Community,15108), (Finland,12348), (Iceland,12347), (Netherlands,12759), (Singapore,12744), (Sweden,12483), (RSA,12446), (Norway,12350), (Denmark,12367), (Poland,12576), (Israel,12512), (Saudi Arabia,12565), (Belgium,12361), (Lithuania,15332), (Greece,12478), (Italy,12349), (France,12413), (Switzerland,12357), (Spain,12354), (USA,12558), (Germany,12426), (United Arab Emirates,12739), (EIRE,14016), (Hong Kong,2147483647), (Bahrain,12353), (Malta,15480), (Unspecified,12363), (Channel Islands,14442), (Austria,12358), (Lebanon,12764), (Czech Republic,12781))\n</code></pre>\n<p><strong>hints:</strong></p>\n<ul>\n  <li>Generate a new KV pair RDD <code>(country, id)</code></li>\n  <li>use <code>rdd.reduceByKey</code> find the smallest</li>\n  <li>ID must be a numric number</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1569247436440_-62527026","id":"20190519-195157_1405617071","dateCreated":"2019-09-23T14:03:56+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:530"},{"text":"\n//your solution\n//you can ignore the output in this section or use it as hints\n//Qustion 1.0 soutuion\n//val splitRdd = retailRDD.map(_.split(\",\"))\n//val UKSamples = splitRdd.filter(_(7).equals(\"United Kingdom\"))\n//printRddNSamples(UKSamples.map(_(6)), 3)\n\nval filterValidCustID = (row: String) => {\n    val tokens = row.split(\",\")\n    !tokens(6).isEmpty\n}\n\nval parseKeyValue = (row: String ) => {\n    val tokens = row.split(\",\")\n    val country= tokens.last\n    val custID = tokens(6).toInt\n    (country, custID)\n    /*val custID:Some[Int] = Some(tokens(6).toInt)\n    (country, custID:Option[Int])*/\n}\n\n\nval ctryRdd = retailRDD.filter(filterValidCustID).map(parseKeyValue)\n\nval min = (a:Int, b:Int) => if (a < b) a else b\nctryRdd.reduceByKey(min).take(5)\n","user":"anonymous","dateUpdated":"2019-10-02T00:00:23+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"filterValidCustID: String => Boolean = <function1>\nparseKeyValue: String => (String, Int) = <function1>\nctryRdd: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[78] at map at <console>:37\nmin: (Int, Int) => Int = <function2>\nres262: Array[(String, Int)] = Array((Australia,12386), (Portugal,12356), (United Kingdom,12346), (Brazil,12769), (Canada,15388))\n"}]},"apps":[],"jobName":"paragraph_1569247436440_-1986749960","id":"20190519-144439_1578143376","dateCreated":"2019-09-23T14:03:56+0000","dateStarted":"2019-10-02T00:00:23+0000","dateFinished":"2019-10-02T00:00:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:531"},{"user":"anonymous","dateUpdated":"2019-09-23T14:03:56+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1569247436441_-1653201930","id":"20190922-215609_1113478409","dateCreated":"2019-09-23T14:03:56+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:532"},{"text":"%md\n\n### Question 2.1\n\nIt's inconvenient to tokenize each row in every operation. Instead, we count convert `retailRDD[String]` to `itemsRdd:RDD[Item]` where `Item` is a case class as following:\n\n`case class Item(invoiceNo:String, stockCode:String, description:Option[String], quantity:Int, invoiceDate:String, unitPrice:Double, customerID:Option[Int], Country:String)`\n\nIn this way, you only parse each row only once here and you can reuse `itemsRdd` in the rest of the questions.\n\n**Sample outputs**\n```scala\n//itemsRdd.take(3)\nres445: Array[Item] = Array(Item(536365,85123A,Some(WHITE HANGING HEART T-LIGHT HOLDER),6,12/1/2010 8:26,2.55,Some(17850),United Kingdom), Item(536365,71053,Some(WHITE METAL LANTERN),6,12/1/2010 8:26,3.39,Some(17850),United Kingdom), Item(536365,84406B,Some(CREAM CUPID HEARTS COAT HANGER),8,12/1/2010 8:26,2.75,Some(17850),United Kingdom))\n```\n\n**Hints**:\n\n- write a function to convert a row to a Item, e.g. \n```\nval parseRow2Item: (String) => Item = (row: String) => {\n    //complete body\n}\n```\n- Covert all rows to items, e.g. `val itemsRdd =  retailRDD.map(parseRow2Item)`\n","user":"anonymous","dateUpdated":"2019-10-01T14:34:37+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Question 2.1</h3>\n<p>It&rsquo;s inconvenient to tokenize each row in every operation. Instead, we count convert <code>retailRDD[String]</code> to <code>itemsRdd:RDD[Item]</code> where <code>Item</code> is a case class as following:</p>\n<p><code>case class Item(invoiceNo:String, stockCode:String, description:Option[String], quantity:Int, invoiceDate:String, unitPrice:Double, customerID:Option[Int], Country:String)</code></p>\n<p>In this way, you only parse each row only once here and you can reuse <code>itemsRdd</code> in the rest of the questions.</p>\n<p><strong>Sample outputs</strong></p>\n<pre><code class=\"scala\">//itemsRdd.take(3)\nres445: Array[Item] = Array(Item(536365,85123A,Some(WHITE HANGING HEART T-LIGHT HOLDER),6,12/1/2010 8:26,2.55,Some(17850),United Kingdom), Item(536365,71053,Some(WHITE METAL LANTERN),6,12/1/2010 8:26,3.39,Some(17850),United Kingdom), Item(536365,84406B,Some(CREAM CUPID HEARTS COAT HANGER),8,12/1/2010 8:26,2.75,Some(17850),United Kingdom))\n</code></pre>\n<p><strong>Hints</strong>:</p>\n<ul>\n  <li>\n  <p>write a function to convert a row to a Item, e.g. </p>\n  <pre><code>val parseRow2Item: (String) =&gt; Item = (row: String) =&gt; {\n//complete body\n}\n</code></pre></li>\n  <li>Covert all rows to items, e.g. <code>val itemsRdd =  retailRDD.map(parseRow2Item)</code></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1569247436442_1875111920","id":"20190921-180308_1963749922","dateCreated":"2019-09-23T14:03:56+0000","dateStarted":"2019-10-01T14:34:37+0000","dateFinished":"2019-10-01T14:34:37+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:533"},{"text":"%spark\n//your solution\n//you can ignore the output in this section or use it as hints\ncase class Item(invoiceNo:String, stockCode:String, description:Option[String], quantity:Int, invoiceDate:String, \n    unitPrice:Double, customer:Option[Int], country:String)\n\n// stackoverflow workaround for customerID NumberFormatException\ndef tryToInt(s: String): Option[Int] = scala.util.Try(s.toInt).toOption\n\nval parseRow2Item: (String) => Item = (row: String) => {\n    val Array(t1, t2, t3, t4, t5, t6, t7, t8) = row.split(\",\")\n    Item(t1, t2, Some(t3), t4.toInt, t5, t6.toDouble, tryToInt(t7), t8)\n}\nval itemsRdd = retailRDD.map(parseRow2Item)\n","user":"anonymous","dateUpdated":"2019-10-03T16:48:53+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"defined class Item\ntryToInt: (s: String)Option[Int]\nparseRow2Item: String => Item = <function1>\nitemsRdd: org.apache.spark.rdd.RDD[Item] = MapPartitionsRDD[5] at map at <console>:39\n"}]},"apps":[],"jobName":"paragraph_1569247436442_151419549","id":"20190921-180433_1776305327","dateCreated":"2019-09-23T14:03:56+0000","dateStarted":"2019-10-03T16:48:53+0000","dateFinished":"2019-10-03T16:48:55+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:534"},{"text":"%spark\nitemsRdd.count","user":"anonymous","dateUpdated":"2019-10-02T01:13:03+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res504: Long = 541909\n"}]},"apps":[],"jobName":"paragraph_1569976028635_-203275887","id":"20191002-002708_1104194812","dateCreated":"2019-10-02T00:27:08+0000","dateStarted":"2019-10-02T01:13:03+0000","dateFinished":"2019-10-02T01:13:04+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:535"},{"text":"%md\n### Questions 2.2\n\nRe-implement questions 1.1 & 2.1 using itemsRdd (`RDD[Item]`)","user":"anonymous","dateUpdated":"2019-09-23T14:03:56+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Questions 2.2</h3>\n<p>Re-implement questions 1.1 &amp; 2.1 using itemsRdd (<code>RDD[Item]</code>)</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1569247436443_1002983829","id":"20190922-140917_1244358721","dateCreated":"2019-09-23T14:03:56+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:536"},{"text":"%spark\n//your solution\n//you can ignore the output in this section or use it as hints\n// retailRDD.count\n// itemsRdd.count\nval itemsGrouped = itemsRdd.\n    map(item => (item.country, (item.quantity*item.unitPrice).toInt)).\n    reduceByKey(_+_).\n    sortBy(_._2, ascending=false).\n    take(5)","user":"anonymous","dateUpdated":"2019-10-02T01:14:29+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"itemsGrouped: Array[(String, Int)] = Array((United Kingdom,7946947), (Netherlands,283853), (EIRE,259901), (Germany,218067))\n"}]},"apps":[],"jobName":"paragraph_1569247436444_639174228","id":"20190922-141025_1178356031","dateCreated":"2019-09-23T14:03:56+0000","dateStarted":"2019-10-02T01:14:08+0000","dateFinished":"2019-10-02T01:14:09+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:537"},{"text":"%spark\n","user":"anonymous","dateUpdated":"2019-10-03T16:45:23+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1570121123164_206586864","id":"20191003-164523_1330858269","dateCreated":"2019-10-03T16:45:23+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:538"},{"text":"%md\n#### Questions 3\n\nFind number of customers.\n\n```\nSELECT distinct(customerId)\nFROM retail\n```\n\n**Sample output**\n```scala\n//resultRdd.count\nres458: Long = 4373\n```","user":"anonymous","dateUpdated":"2019-09-23T14:03:56+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Questions 3</h4>\n<p>Find number of customers.</p>\n<pre><code>SELECT distinct(customerId)\nFROM retail\n</code></pre>\n<p><strong>Sample output</strong></p>\n<pre><code class=\"scala\">//resultRdd.count\nres458: Long = 4373\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1569247436445_417334259","id":"20190519-195407_556558321","dateCreated":"2019-09-23T14:03:56+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:539"},{"text":"%spark\n//your solution\n//you can ignore the output in this section or use it as hints\nitemsRdd.map(_.customer).distinct.count\n","user":"anonymous","dateUpdated":"2019-10-03T16:51:20+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res35: Long = 4373\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://mazh-jrvs-bootcamp-m.us-east1-b.c.rich-suprstate-244018.internal:4040/jobs/job?id=6"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1569247436445_1600791277","id":"20190519-194831_1486531342","dateCreated":"2019-09-23T14:03:56+0000","dateStarted":"2019-10-03T16:51:20+0000","dateFinished":"2019-10-03T16:51:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:540"},{"user":"anonymous","dateUpdated":"2019-09-23T14:03:56+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1569247436446_1784896758","id":"20190922-215651_1847295545","dateCreated":"2019-09-23T14:03:56+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:541"},{"text":"%md\n#### Question 4\n\nFind out the number of invoices/purchases for each customer.\n> Note: a invoiceNo can appear in multiple rows (e.g. a invoice contains more than one item)\n\n```sql\nSELECT CustomerId, count(distinct(invoiceNo)) as purchases\nFROM retail\nGROUP by CustomerId\n```\n\n**Sample output**\n```scala\n//rdd.takeSample(false,3,3)\n//where (key=customerId, value=purchases) for each tuple\nres509: Array[(Any, Int)] = Array((16305,1), (16804,3), (16226,3))\n```\n","user":"anonymous","dateUpdated":"2019-09-23T14:03:56+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Question 4</h4>\n<p>Find out the number of invoices/purchases for each customer.</p>\n<blockquote>\n  <p>Note: a invoiceNo can appear in multiple rows (e.g. a invoice contains more than one item)</p>\n</blockquote>\n<pre><code class=\"sql\">SELECT CustomerId, count(distinct(invoiceNo)) as purchases\nFROM retail\nGROUP by CustomerId\n</code></pre>\n<p><strong>Sample output</strong></p>\n<pre><code class=\"scala\">//rdd.takeSample(false,3,3)\n//where (key=customerId, value=purchases) for each tuple\nres509: Array[(Any, Int)] = Array((16305,1), (16804,3), (16226,3))\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1569247436446_-1968326060","id":"20190519-195310_661372203","dateCreated":"2019-09-23T14:03:56+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:542"},{"text":"%spark\n//your solution\n//you can ignore the output in this section or use it as hints\nitemsRdd.map(item => (item.customer, item.invoiceNo)).\n    distinct.\n    mapValues(_ => 1).\n    reduceByKey(_+_).\n    takeSample(false, 3, 3)\n\n/* verify results\nitemsRdd.map(item => (item.customer, item.invoiceNo)).\n    distinct.\n    filter(_._1 == Some(16804)).\n    mapValues(_ => 1).\n    reduceByKey(_+_).\n    take(1)\n*/","user":"anonymous","dateUpdated":"2019-10-03T17:46:31+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res112: Array[(Option[Int], Int)] = Array((Some(14451),2), (Some(16581),2), (Some(13157),5))\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://mazh-jrvs-bootcamp-m.us-east1-b.c.rich-suprstate-244018.internal:4040/jobs/job?id=29","http://mazh-jrvs-bootcamp-m.us-east1-b.c.rich-suprstate-244018.internal:4040/jobs/job?id=30"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1569247436447_949415943","id":"20190922-190215_1478690977","dateCreated":"2019-09-23T14:03:56+0000","dateStarted":"2019-10-03T17:46:32+0000","dateFinished":"2019-10-03T17:46:34+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:543"},{"text":"%md\n#### SPARK UI\nRun previous paragrah and then use `Spark UI` to inspect Spark jobs \n(click the `SPAKR JOBS` buttom on the top right of the paragraph)","user":"anonymous","dateUpdated":"2019-09-23T14:03:56+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>SPARK UI</h4>\n<p>Run previous paragrah and then use <code>Spark UI</code> to inspect Spark jobs<br/>(click the <code>SPAKR JOBS</code> buttom on the top right of the paragraph)</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1569247436447_-173168458","id":"20190521-114127_1095254606","dateCreated":"2019-09-23T14:03:56+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:544"},{"text":"%md\n#### RDD join\n\nPrint `customerId, name, country` using `customers.txt` and  `online-retail-dataset_clean.txt` files\n\n1. Load `datasets/online_retail/customers.txt` to `RDD[Customer]` where `Customer` is a case class as following\n`case class Customer(customerId:Int, name: String)`\n2. Join `RDD[Customer]` with `RDD[item]` on `customerId`\n3. Select uniq `customerId, name, country`\n\n```sql\nSELECT retail.customerID, customer.name, country\nFROM customer LEFT JOIN retail ON customer.id = retail.customerId\n```\n\n**Sample Output**\n```bash\n//resultRdd.take(3)\nres553: Array[(Int, String, String)] = Array((13311,Petra M. Dalton,United Kingdom), (12367,Wade S. Blair,Denmark), (18144,Gage A. Sharpe,United Kingdom))\n```","user":"anonymous","dateUpdated":"2019-10-03T22:18:21+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>RDD join</h4>\n<p>Print <code>customerId, name, country</code> using <code>customers.txt</code> and <code>online-retail-dataset_clean.txt</code> files</p>\n<ol>\n  <li>Load <code>datasets/online_retail/customers.txt</code> to <code>RDD[Customer]</code> where <code>Customer</code> is a case class as following<br/><code>case class Customer(customerId:Int, name: String)</code></li>\n  <li>Join <code>RDD[Customer]</code> with <code>RDD[item]</code> on <code>customerId</code></li>\n  <li>Select uniq <code>customerId, name, country</code></li>\n</ol>\n<pre><code class=\"sql\">SELECT retail.customerID, customer.name, country\nFROM customer LEFT JOIN retail ON customer.id = retail.customerId\n</code></pre>\n<p><strong>Sample Output</strong></p>\n<pre><code class=\"bash\">//resultRdd.take(3)\nres553: Array[(Int, String, String)] = Array((13311,Petra M. Dalton,United Kingdom), (12367,Wade S. Blair,Denmark), (18144,Gage A. Sharpe,United Kingdom))\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1569247436448_1698592432","id":"20190519-183851_617743118","dateCreated":"2019-09-23T14:03:56+0000","dateStarted":"2019-10-03T22:18:21+0000","dateFinished":"2019-10-03T22:18:23+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:545"},{"text":"%spark\n\ncase class Customer(customerId:Int, name: String)\n\n//your solution\n//you can ignore the output in this section or use it as hints\n\n/* Load RDD */\nval parseRow2Item: (String) => Customer = (row: String) => {\n    val Array(t1, t2) = row.split(\",\")\n    Customer(t1.toInt, t2)\n}\nval customersRDD = sc.textFile(\"hdfs:///user/milad/datasets/online_retail/customers_clean.txt\").\n        map(parseRow2Item)","user":"anonymous","dateUpdated":"2019-10-03T23:35:11+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1570145690422_1881535255","id":"20191003-233450_1399096835","dateCreated":"2019-10-03T23:34:50+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4770"},{"text":"%spark\n//your solution\n//you can ignore the output in this section or use it as hints\n\n/* Join RDD[Customer] with RDD[item] on customerId */\n/* Select uniq customerId, name, country */\nval ctryRdd = itemsRdd.map(i => (i.customer.getOrElse(0), i.country)).distinct\nval namesRdd = customersRDD.map(c => (c.customerId, c.name)).distinct\nval joinRdd = namesRdd.join(ctryRdd)\n/* verify join */\njoinRdd.filter(_._1 == 13311).first\n/* final results */\njoinRdd.map{ \n    case (id, (name, ctry)) => (id, name, ctry)\n}.take(3)\n","user":"anonymous","dateUpdated":"2019-10-04T00:05:11+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"ctryRdd: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[456] at distinct at <console>:46\nnamesRdd: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[460] at distinct at <console>:31\njoinRdd: org.apache.spark.rdd.RDD[(Int, (String, String))] = MapPartitionsRDD[463] at join at <console>:49\nres408: (Int, (String, String)) = (13311,(Petra M. Dalton,United Kingdom))\nres410: Array[(Int, String, String)] = Array((15930,Philip V. Bradford,United Kingdom), (17796,Alvin V. Ellison,United Kingdom), (13732,Sydnee Q. Finley,United Kingdom))\n"}]},"apps":[],"jobName":"paragraph_1569247436449_-1823326045","id":"20190922-193513_1093352183","dateCreated":"2019-09-23T14:03:56+0000","dateStarted":"2019-10-04T00:03:10+0000","dateFinished":"2019-10-04T00:03:13+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:546","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://mazh-jrvs-bootcamp-m.us-east1-b.c.rich-suprstate-244018.internal:4040/jobs/job?id=66","http://mazh-jrvs-bootcamp-m.us-east1-b.c.rich-suprstate-244018.internal:4040/jobs/job?id=67","http://mazh-jrvs-bootcamp-m.us-east1-b.c.rich-suprstate-244018.internal:4040/jobs/job?id=68"],"interpreterSettingId":"spark"}}}],"name":"Jarvis/1-SparkRDD_milad","id":"2EQS3JXM2","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"sh:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}